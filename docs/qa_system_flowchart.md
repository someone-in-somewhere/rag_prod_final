# BÃ¡o CÃ¡o Luá»“ng Há»i ÄÃ¡p Há»‡ Thá»‘ng RAG Chatbot

## Má»¥c Lá»¥c
1. [Tá»•ng Quan](#1-tá»•ng-quan)
2. [Kiáº¿n TrÃºc Há»‡ Thá»‘ng](#2-kiáº¿n-trÃºc-há»‡-thá»‘ng)
3. [Flowchart Luá»“ng Há»i ÄÃ¡p](#3-flowchart-luá»“ng-há»i-Ä‘Ã¡p)
4. [Chi Tiáº¿t Tá»«ng BÆ°á»›c](#4-chi-tiáº¿t-tá»«ng-bÆ°á»›c)
5. [CÃ¡c ThÃ nh Pháº§n ChÃ­nh](#5-cÃ¡c-thÃ nh-pháº§n-chÃ­nh)
6. [Cáº¥u HÃ¬nh Há»‡ Thá»‘ng](#6-cáº¥u-hÃ¬nh-há»‡-thá»‘ng)

---

## 1. Tá»•ng Quan

Há»‡ thá»‘ng RAG (Retrieval-Augmented Generation) Chatbot lÃ  má»™t há»‡ thá»‘ng há»i Ä‘Ã¡p thÃ´ng minh Ä‘Æ°á»£c xÃ¢y dá»±ng Ä‘á»ƒ há»— trá»£ tráº£ lá»i cÃ¡c cÃ¢u há»i vá» láº­p trÃ¬nh nhÃºng (embedded programming). Há»‡ thá»‘ng sá»­ dá»¥ng ká»¹ thuáº­t RAG Ä‘á»ƒ káº¿t há»£p:

- **Retrieval (Truy xuáº¥t)**: TÃ¬m kiáº¿m thÃ´ng tin liÃªn quan tá»« cÆ¡ sá»Ÿ tri thá»©c
- **Augmented (TÄƒng cÆ°á»ng)**: Bá»• sung ngá»¯ cáº£nh cho cÃ¢u há»i
- **Generation (Sinh vÄƒn báº£n)**: Táº¡o cÃ¢u tráº£ lá»i dá»±a trÃªn ngá»¯ cáº£nh

### Äáº·c Äiá»ƒm ChÃ­nh
- Há»— trá»£ Ä‘a ngÃ´n ngá»¯: Tiáº¿ng Viá»‡t vÃ  Tiáº¿ng Anh
- TÃ¬m kiáº¿m lai (Hybrid Search): Káº¿t há»£p Dense + Sparse vectors
- Há»— trá»£ nhiá»u Ä‘á»‹nh dáº¡ng tÃ i liá»‡u: PDF, DOCX, TXT, Images
- Streaming response cho tráº£i nghiá»‡m ngÆ°á»i dÃ¹ng tá»‘t hÆ¡n

---

## 2. Kiáº¿n TrÃºc Há»‡ Thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           FRONTEND (Web UI)                             â”‚
â”‚                         frontend/index.html                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ HTTP POST /chat
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FASTAPI SERVER (server.py)                       â”‚
â”‚                    Host: 0.0.0.0 | Port: 8081                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RAG PIPELINE (rag_pipeline.py)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Language Detection    â€¢ Query Caching    â€¢ Prompt Building          â”‚
â”‚  â€¢ Context Formatting    â€¢ Response Generation                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                      â”‚                       â”‚
           â–¼                      â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    EMBEDDER      â”‚   â”‚   VECTOR STORE   â”‚   â”‚      LLM (vLLM)          â”‚
â”‚  (embedder.py)   â”‚   â”‚ (vectorstore_    â”‚   â”‚  Qwen2.5-7B-Instruct     â”‚
â”‚   BGE-M3 Model   â”‚   â”‚   chroma.py)     â”‚   â”‚  Port: 8000              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    ChromaDB      â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Flowchart Luá»“ng Há»i ÄÃ¡p

### 3.1 Flowchart Tá»•ng Quan (Mermaid)

```mermaid
flowchart TD
    subgraph Input["ğŸ“¥ INPUT"]
        A[ğŸ‘¤ NgÆ°á»i dÃ¹ng gá»­i cÃ¢u há»i]
    end

    subgraph API["ğŸŒ API LAYER"]
        B[POST /chat endpoint]
        C{Kiá»ƒm tra tham sá»‘}
    end

    subgraph Processing["âš™ï¸ Xá»¬ LÃ CÃ‚U Há»I"]
        D[PhÃ¡t hiá»‡n ngÃ´n ngá»¯<br/>vi/en]
        E{Kiá»ƒm tra Cache}
        F[Táº¡o embedding cho query<br/>BGE-M3 Model]
    end

    subgraph Retrieval["ğŸ” TÃŒM KIáº¾M"]
        G[Dense Search<br/>ChromaDB HNSW]
        H[Sparse Search<br/>BM25-like]
        I[Káº¿t há»£p Ä‘iá»ƒm sá»‘<br/>0.7Ã—dense + 0.3Ã—sparse]
        J{Lá»c theo ngÆ°á»¡ng<br/>score â‰¥ 0.4}
    end

    subgraph Context["ğŸ“‹ CHUáº¨N Bá»Š NGá»® Cáº¢NH"]
        K[Format context<br/>vá»›i source citations]
        L[XÃ¢y dá»±ng prompt<br/>System + User messages]
    end

    subgraph Generation["ğŸ¤– SINH CÃ‚U TRáº¢ Lá»œI"]
        M[Gá»­i Ä‘áº¿n LLM<br/>vLLM Server]
        N{Kiá»ƒm tra<br/>NO_RELEVANT_INFO}
        O[Táº¡o response object]
    end

    subgraph Output["ğŸ“¤ OUTPUT"]
        P[Tráº£ vá» JSON response]
        Q[ğŸ‘¤ Hiá»ƒn thá»‹ cho ngÆ°á»i dÃ¹ng]
    end

    A --> B
    B --> C
    C -->|Valid| D
    C -->|Invalid| R[âŒ Error 422]
    D --> E
    E -->|Cache Hit| K
    E -->|Cache Miss| F
    F --> G
    F --> H
    G --> I
    H --> I
    I --> J
    J -->|CÃ³ káº¿t quáº£| K
    J -->|KhÃ´ng cÃ³| S[ğŸ’¬ KhÃ´ng cÃ³ thÃ´ng tin]
    K --> L
    L --> M
    M --> N
    N -->|CÃ³| S
    N -->|KhÃ´ng| O
    O --> P
    S --> P
    P --> Q
```

### 3.2 Flowchart Chi Tiáº¿t - QuÃ¡ TrÃ¬nh Retrieval

```mermaid
flowchart TD
    subgraph Query["ğŸ”¤ QUERY EMBEDDING"]
        A[CÃ¢u há»i ngÆ°á»i dÃ¹ng] --> B[BGE-M3 Encoder]
        B --> C[Dense Vector<br/>1024 dimensions]
        B --> D[Sparse Vector<br/>Lexical weights]
    end

    subgraph Dense["ğŸ¯ DENSE SEARCH"]
        C --> E[ChromaDB Query]
        E --> F[HNSW Index Search]
        F --> G[Cosine Similarity]
        G --> H[Top-K Ã— 2 Results]
    end

    subgraph Sparse["ğŸ“ SPARSE SEARCH"]
        D --> I[Inverted Index Lookup]
        I --> J[Token Matching]
        J --> K[BM25-like Scoring]
        K --> L[Top-K Ã— 2 Results]
    end

    subgraph Fusion["ğŸ”€ SCORE FUSION"]
        H --> M[Normalize Dense Scores]
        L --> N[Normalize Sparse Scores]
        M --> O[Combined Score =<br/>0.7 Ã— dense + 0.3 Ã— sparse]
        N --> O
        O --> P[Sort by Score DESC]
        P --> Q[Select Top-K]
    end

    subgraph Filter["âœ… FILTERING"]
        Q --> R{Score â‰¥ 0.4?}
        R -->|Yes| S[âœ“ Relevant Document]
        R -->|No| T[âœ— Filtered Out]
    end
```

### 3.3 Flowchart Chi Tiáº¿t - QuÃ¡ TrÃ¬nh Generation

```mermaid
flowchart TD
    subgraph Input["ğŸ“¥ INPUT"]
        A[Retrieved Documents]
        B[User Query]
        C[Detected Language]
    end

    subgraph Context["ğŸ“‹ CONTEXT FORMATTING"]
        A --> D[Filter by Relevance]
        D --> E[Format Each Chunk]
        E --> F["[1] Source: file.pdf<br/>(chunk 5, score: 0.87)<br/>content..."]
    end

    subgraph Prompt["ğŸ’¬ PROMPT BUILDING"]
        C --> G{Language?}
        G -->|vi| H[Vietnamese System Prompt]
        G -->|en| I[English System Prompt]
        H --> J[Combine Prompts]
        I --> J
        F --> J
        B --> J
    end

    subgraph LLM["ğŸ¤– LLM INFERENCE"]
        J --> K[Send to vLLM Server]
        K --> L{Success?}
        L -->|No| M[Retry with Backoff]
        M --> N{Retries < 3?}
        N -->|Yes| K
        N -->|No| O[âŒ Error]
        L -->|Yes| P[Parse Response]
    end

    subgraph Validation["âœ”ï¸ VALIDATION"]
        P --> Q{Contains<br/>NO_RELEVANT_INFO?}
        Q -->|Yes| R[Return No Info Message]
        Q -->|No| S[âœ“ Valid Response]
    end

    subgraph Output["ğŸ“¤ OUTPUT"]
        S --> T[Build ChatResponse]
        R --> T
        T --> U[Return to Client]
    end
```

---

## 4. Chi Tiáº¿t Tá»«ng BÆ°á»›c

### BÆ°á»›c 1: Tiáº¿p Nháº­n CÃ¢u Há»i (Request Reception)

**File:** `server.py` (lines 197-225)

**Endpoint:** `POST /chat`

```python
class ChatRequest(BaseModel):
    query: str              # CÃ¢u há»i (1-2000 kÃ½ tá»±)
    top_k: int = 5          # Sá»‘ lÆ°á»£ng documents truy xuáº¥t (1-20)
    max_tokens: int = 1024  # Sá»‘ token tá»‘i Ä‘a cho response
    stream: bool = False    # Báº­t/táº¯t streaming
    use_hybrid: bool = True # Sá»­ dá»¥ng tÃ¬m kiáº¿m lai
```

### BÆ°á»›c 2: PhÃ¡t Hiá»‡n NgÃ´n Ngá»¯ (Language Detection)

**File:** `rag_pipeline.py` (line 199)

Há»‡ thá»‘ng tá»± Ä‘á»™ng phÃ¡t hiá»‡n ngÃ´n ngá»¯ dá»±a trÃªn kÃ½ tá»± tiáº¿ng Viá»‡t:
- Äáº¿m sá»‘ kÃ½ tá»± tiáº¿ng Viá»‡t trong cÃ¢u há»i
- Náº¿u > 2 kÃ½ tá»± â†’ Tiáº¿ng Viá»‡t (`"vi"`)
- NgÆ°á»£c láº¡i â†’ Tiáº¿ng Anh (`"en"`)

### BÆ°á»›c 3: Kiá»ƒm Tra Cache

**File:** `rag_pipeline.py` (lines 66-93)

- Táº¡o cache key tá»« MD5 hash cá»§a (query, top_k, use_hybrid)
- Náº¿u tÃ¬m tháº¥y trong cache â†’ Tráº£ vá» káº¿t quáº£ ngay
- LRU Cache vá»›i kÃ­ch thÆ°á»›c máº·c Ä‘á»‹nh: 1000 entries

### BÆ°á»›c 4: Táº¡o Embedding

**File:** `embedder.py`

Sá»­ dá»¥ng model **BGE-M3** Ä‘á»ƒ táº¡o:
- **Dense Vector**: 1024 chiá»u (semantic similarity)
- **Sparse Vector**: Lexical weights (keyword matching)

```python
result = model.encode(
    [query],
    return_dense=True,
    return_sparse=True,
    return_colbert_vecs=False
)
```

### BÆ°á»›c 5: TÃ¬m Kiáº¿m Lai (Hybrid Search)

**File:** `vectorstore_chroma.py` (lines 113-207)

#### 5.1 Dense Search
- Sá»­ dá»¥ng ChromaDB vá»›i HNSW index
- TÃ­nh cosine similarity
- Láº¥y top_k Ã— 2 káº¿t quáº£

#### 5.2 Sparse Search
- Sá»­ dá»¥ng inverted index in-memory
- TÃ­nh BM25-like score
- Láº¥y top_k Ã— 2 káº¿t quáº£

#### 5.3 Score Fusion
```python
combined_score = 0.7 Ã— dense_score + 0.3 Ã— sparse_score
```

#### 5.4 Filtering
- Lá»c cÃ¡c documents cÃ³ score â‰¥ 0.4 (ngÆ°á»¡ng relevance)

### BÆ°á»›c 6: Chuáº©n Bá»‹ Ngá»¯ Cáº£nh (Context Preparation)

**File:** `rag_pipeline.py` (lines 103-129)

Format má»—i document thÃ nh:
```
[1] Source: filename.pdf (chunk 5, relevance: 0.87)
<ná»™i dung chunk>

---

[2] Source: filename2.pdf (chunk 12, relevance: 0.75)
<ná»™i dung chunk>
```

### BÆ°á»›c 7: XÃ¢y Dá»±ng Prompt

**File:** `rag_pipeline.py` (lines 132-156)

**System Prompt (Tiáº¿ng Viá»‡t):**
```
Báº¡n lÃ  trá»£ lÃ½ chuyÃªn gia vá» láº­p trÃ¬nh nhÃºng.

QUY Táº®C QUAN TRá»ŒNG:
- CHá»ˆ tráº£ lá»i dá»±a trÃªn ngá»¯ cáº£nh
- Náº¿u khÃ´ng cÃ³ thÃ´ng tin, tráº£ lá»i: "NO_RELEVANT_INFO"
- KHÃ”NG ÄÆ¯á»¢C bá»‹a hoáº·c suy luáº­n
- LuÃ´n trÃ­ch dáº«n nguá»“n
```

**Prompt Structure:**
```json
[
  {"role": "system", "content": "<system prompt>"},
  {"role": "user", "content": "Context:...\nQuestion:..."}
]
```

### BÆ°á»›c 8: Sinh CÃ¢u Tráº£ Lá»i (LLM Generation)

**File:** `rag_pipeline.py` (lines 159-185)

- **Model:** Qwen2.5-7B-Instruct
- **Server:** vLLM (http://localhost:8000/v1)
- **Temperature:** 0.7
- **Max Tokens:** 1024 (cÃ³ thá»ƒ cáº¥u hÃ¬nh)
- **Retry:** Tá»‘i Ä‘a 3 láº§n vá»›i exponential backoff

### BÆ°á»›c 9: Tráº£ Vá» Response

**File:** `rag_pipeline.py` (lines 265-279)

```json
{
  "query": "CÃ¢u há»i gá»‘c",
  "response": "CÃ¢u tráº£ lá»i tá»« LLM",
  "language": "vi",
  "sources": [
    {"source": "file.pdf", "score": 0.87, "chunk_index": 5}
  ],
  "context_used": true,
  "retrieval_info": {
    "docs_found": 10,
    "docs_relevant": 3,
    "retrieve_time_ms": 125,
    "generate_time_ms": 2340,
    "total_time_ms": 2465,
    "hybrid_search": true
  }
}
```

---

## 5. CÃ¡c ThÃ nh Pháº§n ChÃ­nh

### 5.1 Báº£ng Tá»•ng Há»£p Files

| File | Chá»©c NÄƒng | MÃ´ Táº£ |
|------|-----------|-------|
| `server.py` | API Server | FastAPI endpoints, request handling |
| `rag_pipeline.py` | RAG Orchestration | Äiá»u phá»‘i toÃ n bá»™ luá»“ng Q&A |
| `vectorstore_chroma.py` | Vector Database | Hybrid search, indexing |
| `embedder.py` | Embedding Service | BGE-M3 model, dense+sparse encoding |
| `document_ingest.py` | Document Processing | PDF/DOCX/text parsing, chunking |
| `redis_store.py` | Document Storage | Raw text persistence |
| `ocr_utils.py` | Image Processing | PaddleOCR, vision captioning |
| `config.py` | Configuration | Tham sá»‘ há»‡ thá»‘ng |

### 5.2 Technology Stack

| ThÃ nh Pháº§n | CÃ´ng Nghá»‡ |
|------------|-----------|
| Backend | FastAPI 0.115+ |
| Vector DB | ChromaDB 0.5+ |
| Embedding | BGE-M3 (BAAI) |
| LLM | Qwen2.5-7B-Instruct |
| Vision | Qwen2-VL-7B |
| OCR | PaddleOCR |
| Cache | Redis 5.0+ |
| Document | PyMuPDF, python-docx |

---

## 6. Cáº¥u HÃ¬nh Há»‡ Thá»‘ng

### 6.1 Retrieval Configuration

| Tham Sá»‘ | GiÃ¡ Trá»‹ Máº·c Äá»‹nh | MÃ´ Táº£ |
|---------|------------------|-------|
| `TOP_K` | 5 | Sá»‘ documents truy xuáº¥t |
| `RELEVANCE_THRESHOLD` | 0.4 | NgÆ°á»¡ng relevance |
| `DENSE_WEIGHT` | 0.7 | Trá»ng sá»‘ dense search |
| `SPARSE_WEIGHT` | 0.3 | Trá»ng sá»‘ sparse search |

### 6.2 Chunking Configuration

| Tham Sá»‘ | GiÃ¡ Trá»‹ Máº·c Äá»‹nh | MÃ´ Táº£ |
|---------|------------------|-------|
| `CHUNK_SIZE` | 512 | KÃ­ch thÆ°á»›c chunk (words) |
| `CHUNK_OVERLAP` | 50 | Overlap giá»¯a chunks |
| `USE_SEMANTIC_CHUNKING` | true | Báº­t semantic chunking |

### 6.3 Generation Configuration

| Tham Sá»‘ | GiÃ¡ Trá»‹ Máº·c Äá»‹nh | MÃ´ Táº£ |
|---------|------------------|-------|
| `MAX_TOKENS` | 1024 | Token tá»‘i Ä‘a |
| `TEMPERATURE` | 0.7 | Äá»™ sÃ¡ng táº¡o |
| `MAX_RETRIES` | 3 | Sá»‘ láº§n retry |
| `RETRY_DELAY` | 1.0s | Thá»i gian chá» giá»¯a retries |

---

## 7. Sequence Diagram

```mermaid
sequenceDiagram
    participant U as ğŸ‘¤ User
    participant S as ğŸŒ Server
    participant R as âš™ï¸ RAG Pipeline
    participant E as ğŸ”¤ Embedder
    participant V as ğŸ—„ï¸ VectorStore
    participant L as ğŸ¤– LLM (vLLM)

    U->>S: POST /chat {query, top_k, ...}
    S->>R: chat(query, top_k, ...)

    R->>R: detect_language(query)
    R->>R: check_cache(query)

    alt Cache Miss
        R->>E: embed_query(query)
        E-->>R: {dense, sparse} vectors

        R->>V: hybrid_search(vectors, top_k)
        V->>V: dense_search (HNSW)
        V->>V: sparse_search (Inverted Index)
        V->>V: score_fusion (0.7Ã—d + 0.3Ã—s)
        V-->>R: retrieved_documents

        R->>R: filter_by_relevance(docs)
        R->>R: save_to_cache()
    end

    R->>R: format_context(docs)
    R->>R: build_prompt(query, context, lang)

    R->>L: generate(messages, max_tokens)
    L-->>R: response_text

    R->>R: validate_response()
    R->>R: build_chat_response()

    R-->>S: ChatResponse
    S-->>U: JSON Response
```

---

## 8. Káº¿t Luáº­n

Há»‡ thá»‘ng RAG Chatbot Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i cÃ¡c Ä‘áº·c Ä‘iá»ƒm:

1. **Hiá»‡u Quáº£**: Sá»­ dá»¥ng caching vÃ  hybrid search Ä‘á»ƒ tá»‘i Æ°u performance
2. **ChÃ­nh XÃ¡c**: Káº¿t há»£p semantic (dense) vÃ  lexical (sparse) search
3. **Äa NgÃ´n Ngá»¯**: Há»— trá»£ tiáº¿ng Viá»‡t vÃ  tiáº¿ng Anh
4. **Trung Thá»±c**: LLM chá»‰ tráº£ lá»i dá»±a trÃªn context, khÃ´ng bá»‹a thÃ´ng tin
5. **CÃ³ TrÃ­ch Dáº«n**: Má»i cÃ¢u tráº£ lá»i Ä‘á»u cÃ³ nguá»“n tham kháº£o

---

*BÃ¡o cÃ¡o Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng - NgÃ y: 2025-12-10*
