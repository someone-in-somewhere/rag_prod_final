"""
RAG Pipeline Module
===================
Module xá»­ lÃ½ RAG (Retrieval-Augmented Generation) vá»›i caching vÃ  retry.

RAG Pipeline Flow:
1. User gá»­i query
2. Detect ngÃ´n ngá»¯ (vi/en)
3. Retrieve: TÃ¬m top-k documents liÃªn quan tá»« vector store
4. Filter: Lá»c documents cÃ³ score >= threshold
5. Build prompt: Táº¡o prompt vá»›i context tá»« retrieved docs
6. Generate: Gá»i LLM Ä‘á»ƒ sinh cÃ¢u tráº£ lá»i
7. Return: Tráº£ vá» response + sources

Caching:
- Query cache: LÆ°u káº¿t quáº£ retrieval theo query hash
- FIFO eviction: Khi cache Ä‘áº§y, xÃ³a entries cÅ© nháº¥t
- Clear cache khi cÃ³ document má»›i Ä‘Æ°á»£c ingest

Retry Logic:
- Generate cÃ³ thá»ƒ fail do network/server issues
- Retry vá»›i exponential backoff (MAX_RETRIES láº§n)

Language Support:
- Tá»± Ä‘á»™ng detect ngÃ´n ngá»¯ tá»« query
- System prompt vÃ  response message theo ngÃ´n ngá»¯

Sá»­ dá»¥ng:
    from rag_pipeline import chat, chat_stream, retrieve

    # Chat thÆ°á»ng
    result = chat("GPIO lÃ  gÃ¬?", top_k=5)
    print(result["response"])

    # Streaming chat
    for chunk in chat_stream("Explain I2C protocol"):
        print(chunk, end="")

    # Debug retrieval
    docs = retrieve("UART configuration")
    for d in docs:
        print(f"{d['score']:.3f}: {d['text'][:100]}...")
"""

from openai import OpenAI
from typing import List, Dict, Optional, Generator
import hashlib
import time
import logging
from datetime import datetime

from vectorstore_chroma import get_vectorstore
from config import (
    VLLM_BASE_URL, LLM_MODEL, TOP_K, RELEVANCE_THRESHOLD,
    DENSE_WEIGHT, SPARSE_WEIGHT, QUERY_CACHE_SIZE, ENABLE_QUERY_CACHE,
    MAX_RETRIES, RETRY_DELAY, TEMPERATURE, LOG_LEVEL,
    DEBUG_RETRIEVAL, DEBUG_GENERATION, DEBUG_CONTEXT
)

# Setup logging
logging.basicConfig(level=getattr(logging, LOG_LEVEL))
logger = logging.getLogger(__name__)


def log_debug(flag: bool, prefix: str, message: str):
    """
    Log debug cÃ³ Ä‘iá»u kiá»‡n.

    Args:
        flag: Debug flag tá»« config (DEBUG_RETRIEVAL, DEBUG_GENERATION, etc.)
        prefix: Prefix cho log (emoji + category)
        message: Ná»™i dung log
    """
    if flag:
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {prefix}: {message}")


# vLLM client - káº¿t ná»‘i Ä‘áº¿n vLLM server qua OpenAI-compatible API
llm_client = OpenAI(base_url=VLLM_BASE_URL, api_key="not-needed")

# Cache cho query embeddings
# Key: hash cá»§a (query, top_k, use_hybrid)
# Value: List[Dict] - retrieved documents
_query_cache: Dict[str, List[Dict]] = {}
MAX_CACHE_SIZE = QUERY_CACHE_SIZE


# ============================================
# System Prompts
# ============================================

SYSTEM_PROMPT_EN = """You are an expert assistant specializing in embedded programming and embedded systems.

IMPORTANT RULES:
- ONLY answer based on the provided context from the knowledge base.
- If the context does not contain relevant information, respond EXACTLY with: "NO_RELEVANT_INFO"
- DO NOT make up or infer information not in the context.
- Always cite which document/source you got the information from.
- Provide code examples only if they exist in the context.
- For technical terms, registers, or configurations, be precise and accurate.

Respond in the same language as the user's question."""

SYSTEM_PROMPT_VI = """Báº¡n lÃ  trá»£ lÃ½ chuyÃªn gia vá» láº­p trÃ¬nh nhÃºng vÃ  há»‡ thá»‘ng nhÃºng.

QUY Táº®C QUAN TRá»ŒNG:
- CHá»ˆ tráº£ lá»i dá»±a trÃªn ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p tá»« cÆ¡ sá»Ÿ kiáº¿n thá»©c.
- Náº¿u ngá»¯ cáº£nh KHÃ”NG chá»©a thÃ´ng tin liÃªn quan, tráº£ lá»i CHÃNH XÃC: "NO_RELEVANT_INFO"
- KHÃ”NG ÄÆ¯á»¢C bá»‹a hoáº·c suy luáº­n thÃ´ng tin khÃ´ng cÃ³ trong ngá»¯ cáº£nh.
- LuÃ´n trÃ­ch dáº«n nguá»“n tÃ i liá»‡u mÃ  báº¡n láº¥y thÃ´ng tin.
- Chá»‰ cung cáº¥p vÃ­ dá»¥ code náº¿u cÃ³ trong ngá»¯ cáº£nh.
- Vá»›i cÃ¡c thuáº­t ngá»¯ ká»¹ thuáº­t, thanh ghi, cáº¥u hÃ¬nh, hÃ£y chÃ­nh xÃ¡c.

Tráº£ lá»i báº±ng ngÃ´n ngá»¯ cá»§a cÃ¢u há»i."""


# ============================================
# Helper Functions
# ============================================

def detect_language(text: str) -> str:
    """
    Detect ngÃ´n ngá»¯ cá»§a text: Tiáº¿ng Viá»‡t hoáº·c Tiáº¿ng Anh.

    PhÆ°Æ¡ng phÃ¡p: Äáº¿m sá»‘ kÃ½ tá»± tiáº¿ng Viá»‡t Ä‘áº·c trÆ°ng.
    Náº¿u cÃ³ > 2 kÃ½ tá»± tiáº¿ng Viá»‡t -> "vi", ngÆ°á»£c láº¡i -> "en"

    Args:
        text: VÄƒn báº£n cáº§n detect

    Returns:
        str: "vi" hoáº·c "en"

    Example:
        >>> detect_language("GPIO lÃ  gÃ¬?")
        'vi'
        >>> detect_language("What is GPIO?")
        'en'
    """
    vn_chars = set("Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘")
    text_lower = text.lower()
    vn_count = sum(1 for c in text_lower if c in vn_chars)
    return "vi" if vn_count > 2 else "en"


def _cache_key(query: str, top_k: int, use_hybrid: bool) -> str:
    """
    Táº¡o cache key cho query.

    Cache key lÃ  MD5 hash cá»§a (query, top_k, use_hybrid) Ä‘á»ƒ:
    - Äáº£m báº£o key ngáº¯n gá»n
    - TrÃ¡nh special characters trong key

    Args:
        query: CÃ¢u query
        top_k: Sá»‘ káº¿t quáº£
        use_hybrid: CÃ³ dÃ¹ng hybrid search khÃ´ng

    Returns:
        str: MD5 hash string (32 chars)
    """
    return hashlib.md5(f"{query}:{top_k}:{use_hybrid}".encode()).hexdigest()


# ============================================
# Retrieval Functions
# ============================================

def retrieve_with_cache(
    query: str,
    top_k: int = TOP_K,
    use_hybrid: bool = True
) -> List[Dict]:
    """
    Retrieve documents vá»›i caching.

    QuÃ¡ trÃ¬nh:
    1. Táº¡o cache key tá»« query params
    2. Check cache, náº¿u hit thÃ¬ return cached results
    3. Náº¿u miss, gá»i vector store search
    4. LÆ°u káº¿t quáº£ vÃ o cache (vá»›i size limit)

    Cache eviction: Simple FIFO - khi cache Ä‘áº§y, xÃ³a 25% entries cÅ© nháº¥t.

    Args:
        query: CÃ¢u query tÃ¬m kiáº¿m
        top_k: Sá»‘ káº¿t quáº£ tá»‘i Ä‘a (default tá»« config)
        use_hybrid: CÃ³ dÃ¹ng hybrid search khÃ´ng (default: True)

    Returns:
        List[Dict]: Danh sÃ¡ch documents tÃ¬m Ä‘Æ°á»£c
        Má»—i dict cÃ³: id, text, score, metadata, (dense_score, sparse_score náº¿u hybrid)
    """
    global _query_cache

    cache_key = _cache_key(query, top_k, use_hybrid)

    # Check cache
    if ENABLE_QUERY_CACHE and cache_key in _query_cache:
        log_debug(DEBUG_RETRIEVAL, "ðŸ” RETRIEVE", f"Cache HIT for: '{query[:50]}...'")
        return _query_cache[cache_key]

    log_debug(DEBUG_RETRIEVAL, "ðŸ” RETRIEVE", f"Cache MISS, searching: '{query[:50]}...'")

    # Retrieve tá»« vector store
    vs = get_vectorstore()
    results = vs.search(query, top_k=top_k, use_hybrid=use_hybrid)

    # Log top results khi DEBUG
    if DEBUG_RETRIEVAL and results:
        log_debug(DEBUG_RETRIEVAL, "ðŸ” RETRIEVE", f"Found {len(results)} docs:")
        for i, doc in enumerate(results[:3]):  # Top 3
            source = doc.get("metadata", {}).get("source", "?")
            score = doc.get("score", 0)
            text_preview = doc.get("text", "")[:80].replace("\n", " ")
            log_debug(
                DEBUG_RETRIEVAL, "ðŸ” RETRIEVE",
                f"  [{i+1}] {score:.3f} | {source} | {text_preview}..."
            )

    # Update cache (vá»›i size limit)
    if ENABLE_QUERY_CACHE:
        if len(_query_cache) >= MAX_CACHE_SIZE:
            # Remove oldest entries (simple FIFO)
            keys_to_remove = list(_query_cache.keys())[:MAX_CACHE_SIZE // 4]
            for k in keys_to_remove:
                del _query_cache[k]
            log_debug(DEBUG_RETRIEVAL, "ðŸ” RETRIEVE", f"Cache eviction: removed {len(keys_to_remove)} entries")

        _query_cache[cache_key] = results

    return results


def clear_cache():
    """
    Clear query cache.

    Gá»i khi:
    - CÃ³ document má»›i Ä‘Æ°á»£c ingest
    - Document bá»‹ xÃ³a
    - User yÃªu cáº§u clear cache

    Side effects:
    - Reset _query_cache vá» empty dict
    """
    global _query_cache
    _query_cache = {}
    logger.info("Query cache cleared")
    log_debug(DEBUG_RETRIEVAL, "ðŸ” RETRIEVE", "Cache cleared")


# ============================================
# Context Formatting
# ============================================

def format_context(docs: List[Dict]) -> tuple:
    """
    Format retrieved docs thÃ nh context string cho LLM.

    QuÃ¡ trÃ¬nh:
    1. Lá»c docs cÃ³ score >= RELEVANCE_THRESHOLD
    2. Format má»—i doc vá»›i source, score, text
    3. Join táº¥t cáº£ docs vá»›i separator

    Args:
        docs: List documents tá»« retrieval

    Returns:
        tuple: (context_string, has_relevant_docs, relevant_docs_list)
        - context_string: Formatted context hoáº·c "No relevant documents found."
        - has_relevant_docs: True náº¿u cÃ³ Ã­t nháº¥t 1 doc relevant
        - relevant_docs_list: List cÃ¡c docs Ä‘Ã£ lá»c

    Example output:
        [1] Source: gpio.pdf (chunk 5, relevance: 0.85)
        GPIO (General Purpose Input/Output) lÃ ...

        ---

        [2] Source: gpio.pdf (chunk 6, relevance: 0.82)
        Äá»ƒ cáº¥u hÃ¬nh GPIO mode...
    """
    if not docs:
        return "No relevant documents found.", False, []

    # Filter theo relevance threshold
    relevant_docs = [d for d in docs if d.get("score", 0) >= RELEVANCE_THRESHOLD]

    if not relevant_docs:
        log_debug(
            DEBUG_CONTEXT, "ðŸ“‹ CONTEXT",
            f"No docs above threshold {RELEVANCE_THRESHOLD}, all scores: "
            f"{[d.get('score', 0):.3f for d in docs[:5]]}"
        )
        return "No relevant documents found.", False, []

    log_debug(
        DEBUG_CONTEXT, "ðŸ“‹ CONTEXT",
        f"Filtered {len(relevant_docs)}/{len(docs)} docs (threshold={RELEVANCE_THRESHOLD})"
    )

    # Format tá»«ng doc
    context_parts = []
    for i, doc in enumerate(relevant_docs, 1):
        source = doc.get("metadata", {}).get("source", "Unknown")
        score = doc.get("score", 0)
        chunk_idx = doc.get("metadata", {}).get("chunk_index", "?")
        text = doc.get("text", "")

        # ThÃªm thÃ´ng tin vá» loáº¡i score náº¿u cÃ³ (hybrid search)
        score_info = f"relevance: {score:.2f}"
        if "dense_score" in doc and "sparse_score" in doc:
            score_info += f", dense: {doc['dense_score']:.2f}, sparse: {doc['sparse_score']:.2f}"

        context_parts.append(
            f"[{i}] Source: {source} (chunk {chunk_idx}, {score_info})\n{text}"
        )

    context_str = "\n\n---\n\n".join(context_parts)

    # Log context khi DEBUG_CONTEXT
    if DEBUG_CONTEXT:
        log_debug(DEBUG_CONTEXT, "ðŸ“‹ CONTEXT", f"Context length: {len(context_str)} chars")
        log_debug(DEBUG_CONTEXT, "ðŸ“‹ CONTEXT", f"Context preview:\n{context_str[:500]}...")

    return context_str, True, relevant_docs


# ============================================
# Prompt Building
# ============================================

def build_prompt(query: str, context: str, lang: str) -> List[Dict]:
    """
    Build prompt messages cho LLM.

    Cáº¥u trÃºc messages:
    1. System message: HÆ°á»›ng dáº«n role vÃ  rules
    2. User message: Context + Question

    Args:
        query: CÃ¢u há»i cá»§a user
        context: Context Ä‘Ã£ format tá»« retrieved docs
        lang: NgÃ´n ngá»¯ ("vi" hoáº·c "en")

    Returns:
        List[Dict]: Messages cho OpenAI-compatible API
        [{"role": "system", "content": ...}, {"role": "user", "content": ...}]
    """
    system = SYSTEM_PROMPT_VI if lang == "vi" else SYSTEM_PROMPT_EN

    if lang == "vi":
        user_content = f"""Ngá»¯ cáº£nh tá»« cÆ¡ sá»Ÿ kiáº¿n thá»©c:
{context}

---
CÃ¢u há»i: {query}

HÃ£y tráº£ lá»i chi tiáº¿t dá»±a trÃªn ngá»¯ cáº£nh. Náº¿u khÃ´ng cÃ³ thÃ´ng tin liÃªn quan, tráº£ lá»i "NO_RELEVANT_INFO"."""
    else:
        user_content = f"""Context from knowledge base:
{context}

---
Question: {query}

Provide a detailed answer based on the context. If no relevant information, respond with "NO_RELEVANT_INFO"."""

    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": user_content}
    ]

    # Log prompt khi DEBUG_GENERATION
    if DEBUG_GENERATION:
        log_debug(DEBUG_GENERATION, "âš¡ GENERATE", f"Prompt length: {len(user_content)} chars")
        log_debug(DEBUG_GENERATION, "âš¡ GENERATE", f"System prompt: {system[:100]}...")

    return messages


# ============================================
# Generation Functions
# ============================================

def generate_with_retry(
    messages: List[Dict],
    max_tokens: int = 1024,
    temperature: float = TEMPERATURE
) -> str:
    """
    Generate response tá»« LLM vá»›i retry logic.

    QuÃ¡ trÃ¬nh:
    1. Gá»i vLLM qua OpenAI-compatible API
    2. Náº¿u fail, retry vá»›i exponential backoff
    3. Sau MAX_RETRIES láº§n fail, raise exception

    Retry delays: RETRY_DELAY * attempt (1s, 2s, 3s, ...)

    Args:
        messages: List messages (system + user)
        max_tokens: Sá»‘ tokens tá»‘i Ä‘a cho response (default: 1024)
        temperature: Sampling temperature (default tá»« config)

    Returns:
        str: Generated response tá»« LLM

    Raises:
        Exception: Náº¿u fail sau MAX_RETRIES láº§n

    Example:
        >>> messages = [{"role": "user", "content": "Hello"}]
        >>> response = generate_with_retry(messages, max_tokens=100)
    """
    last_error = None

    log_debug(
        DEBUG_GENERATION, "âš¡ GENERATE",
        f"Calling LLM: model={LLM_MODEL}, max_tokens={max_tokens}, temp={temperature}"
    )

    for attempt in range(MAX_RETRIES):
        try:
            start_time = time.time()

            response = llm_client.chat.completions.create(
                model=LLM_MODEL,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9
            )

            result = response.choices[0].message.content
            elapsed = time.time() - start_time

            log_debug(
                DEBUG_GENERATION, "âš¡ GENERATE",
                f"Response received: {len(result)} chars in {elapsed:.2f}s"
            )

            # Log response preview
            if DEBUG_GENERATION:
                log_debug(
                    DEBUG_GENERATION, "âš¡ GENERATE",
                    f"Response preview: {result[:200]}..."
                )

            return result

        except Exception as e:
            last_error = e
            logger.warning(f"Generation attempt {attempt + 1} failed: {e}")
            log_debug(
                DEBUG_GENERATION, "âš¡ GENERATE",
                f"Attempt {attempt + 1} failed: {e}"
            )

            if attempt < MAX_RETRIES - 1:
                sleep_time = RETRY_DELAY * (attempt + 1)
                log_debug(DEBUG_GENERATION, "âš¡ GENERATE", f"Retrying in {sleep_time}s...")
                time.sleep(sleep_time)

    logger.error(f"Generation failed after {MAX_RETRIES} attempts: {last_error}")
    raise last_error


# ============================================
# Main Chat Functions
# ============================================

def chat(
    query: str,
    top_k: int = TOP_K,
    max_tokens: int = 1024,
    use_hybrid: bool = True
) -> Dict:
    """
    Main chat function - Xá»­ lÃ½ query vÃ  tráº£ vá» response.

    RAG Pipeline:
    1. Detect ngÃ´n ngá»¯
    2. Retrieve top-k docs tá»« vector store
    3. Filter docs theo relevance threshold
    4. Build prompt vá»›i context
    5. Generate response tá»« LLM
    6. Return káº¿t quáº£ vá»›i metadata

    Args:
        query: CÃ¢u há»i cá»§a user
        top_k: Sá»‘ docs retrieve (default tá»« config)
        max_tokens: Max tokens cho response (default: 1024)
        use_hybrid: CÃ³ dÃ¹ng hybrid search khÃ´ng (default: True)

    Returns:
        Dict vá»›i cÃ¡c keys:
        - query: CÃ¢u query gá»‘c
        - response: CÃ¢u tráº£ lá»i tá»« LLM
        - language: "vi" hoáº·c "en"
        - sources: List sources Ä‘Æ°á»£c sá»­ dá»¥ng
        - context_used: True náº¿u cÃ³ sá»­ dá»¥ng context
        - retrieval_info: Dict vá»›i timing vÃ  stats:
            - docs_found: Sá»‘ docs tÃ¬m Ä‘Æ°á»£c
            - docs_relevant: Sá»‘ docs vÆ°á»£t threshold
            - retrieve_time_ms: Thá»i gian retrieve (ms)
            - generate_time_ms: Thá»i gian generate (ms)
            - total_time_ms: Tá»•ng thá»i gian (ms)
            - hybrid_search: CÃ³ dÃ¹ng hybrid khÃ´ng

    Example:
        >>> result = chat("GPIO lÃ  gÃ¬?")
        >>> print(result["response"])
        GPIO (General Purpose Input/Output) lÃ  cÃ¡c chÃ¢n Ä‘a nÄƒng...
        >>> print(result["sources"])
        [{"source": "stm32.pdf", "score": 0.85, "chunk_index": 5}]
    """
    start_time = time.time()
    lang = detect_language(query)

    log_debug(DEBUG_GENERATION, "âš¡ CHAT", f"Query: '{query[:80]}...', lang={lang}")

    # Retrieve
    retrieve_start = time.time()
    retrieved_docs = retrieve_with_cache(query, top_k=top_k, use_hybrid=use_hybrid)
    retrieve_time = time.time() - retrieve_start

    log_debug(
        DEBUG_RETRIEVAL, "ðŸ” RETRIEVE",
        f"Retrieved {len(retrieved_docs)} docs in {retrieve_time*1000:.0f}ms"
    )

    # Format context
    context, has_relevant, relevant_docs = format_context(retrieved_docs)

    # No info message theo ngÃ´n ngá»¯
    no_info_msg = (
        "TÃ´i khÃ´ng cÃ³ thÃ´ng tin vá» chá»§ Ä‘á» nÃ y trong tÃ i liá»‡u hiá»‡n táº¡i. "
        "Vui lÃ²ng upload tÃ i liá»‡u liÃªn quan hoáº·c há»i cÃ¢u há»i khÃ¡c."
    ) if lang == "vi" else (
        "I don't have information about this topic in the current documents. "
        "Please upload relevant documents or ask another question."
    )

    # Náº¿u khÃ´ng cÃ³ context relevant
    if not has_relevant:
        log_debug(DEBUG_GENERATION, "âš¡ CHAT", "No relevant context, returning no_info message")
        return {
            "query": query,
            "response": no_info_msg,
            "language": lang,
            "sources": [],
            "context_used": False,
            "retrieval_info": {
                "docs_found": len(retrieved_docs),
                "docs_relevant": 0,
                "retrieve_time_ms": int(retrieve_time * 1000),
                "hybrid_search": use_hybrid
            }
        }

    # Generate
    generate_start = time.time()
    messages = build_prompt(query, context, lang)
    response = generate_with_retry(messages, max_tokens=max_tokens)
    generate_time = time.time() - generate_start

    log_debug(
        DEBUG_GENERATION, "âš¡ GENERATE",
        f"Generated response in {generate_time*1000:.0f}ms"
    )

    # Check if model says no info
    if "NO_RELEVANT_INFO" in response:
        log_debug(DEBUG_GENERATION, "âš¡ CHAT", "LLM returned NO_RELEVANT_INFO")
        return {
            "query": query,
            "response": no_info_msg,
            "language": lang,
            "sources": [],
            "context_used": False,
            "retrieval_info": {
                "docs_found": len(retrieved_docs),
                "docs_relevant": len(relevant_docs),
                "retrieve_time_ms": int(retrieve_time * 1000),
                "generate_time_ms": int(generate_time * 1000),
                "hybrid_search": use_hybrid
            }
        }

    # Build sources list
    sources = [{
        "source": d.get("metadata", {}).get("source"),
        "score": round(d.get("score", 0), 3),
        "chunk_index": d.get("metadata", {}).get("chunk_index")
    } for d in relevant_docs]

    total_time = time.time() - start_time

    log_debug(
        DEBUG_GENERATION, "âš¡ CHAT",
        f"Chat completed: {total_time*1000:.0f}ms total, "
        f"{len(sources)} sources used"
    )

    return {
        "query": query,
        "response": response,
        "language": lang,
        "sources": sources,
        "context_used": True,
        "retrieval_info": {
            "docs_found": len(retrieved_docs),
            "docs_relevant": len(relevant_docs),
            "retrieve_time_ms": int(retrieve_time * 1000),
            "generate_time_ms": int(generate_time * 1000),
            "total_time_ms": int(total_time * 1000),
            "hybrid_search": use_hybrid
        }
    }


def chat_stream(
    query: str,
    top_k: int = TOP_K,
    max_tokens: int = 1024,
    use_hybrid: bool = True
) -> Generator[str, None, None]:
    """
    Streaming chat - Tráº£ vá» response tá»«ng chunk.

    TÆ°Æ¡ng tá»± chat() nhÆ°ng yield tá»«ng token thay vÃ¬ tráº£ vá» toÃ n bá»™.
    DÃ¹ng cho real-time display trong UI.

    Args:
        query: CÃ¢u há»i cá»§a user
        top_k: Sá»‘ docs retrieve
        max_tokens: Max tokens cho response
        use_hybrid: CÃ³ dÃ¹ng hybrid search khÃ´ng

    Yields:
        str: Tá»«ng chunk cá»§a response

    Example:
        >>> for chunk in chat_stream("GPIO lÃ  gÃ¬?"):
        ...     print(chunk, end="", flush=True)
    """
    lang = detect_language(query)

    log_debug(DEBUG_GENERATION, "âš¡ STREAM", f"Stream query: '{query[:50]}...'")

    # Retrieve
    retrieved_docs = retrieve_with_cache(query, top_k=top_k, use_hybrid=use_hybrid)
    context, has_relevant, relevant_docs = format_context(retrieved_docs)

    if not has_relevant:
        no_info_msg = (
            "TÃ´i khÃ´ng cÃ³ thÃ´ng tin vá» chá»§ Ä‘á» nÃ y trong tÃ i liá»‡u hiá»‡n táº¡i. "
            "Vui lÃ²ng upload tÃ i liá»‡u liÃªn quan hoáº·c há»i cÃ¢u há»i khÃ¡c."
        ) if lang == "vi" else (
            "I don't have information about this topic in the current documents. "
            "Please upload relevant documents or ask another question."
        )
        yield no_info_msg
        return

    messages = build_prompt(query, context, lang)

    try:
        log_debug(DEBUG_GENERATION, "âš¡ STREAM", "Starting stream generation...")

        response = llm_client.chat.completions.create(
            model=LLM_MODEL,
            messages=messages,
            max_tokens=max_tokens,
            temperature=TEMPERATURE,
            stream=True
        )

        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

        log_debug(DEBUG_GENERATION, "âš¡ STREAM", "Stream completed")

    except Exception as e:
        logger.error(f"Stream generation error: {e}")
        log_debug(DEBUG_GENERATION, "âš¡ STREAM", f"Stream error: {e}")
        yield f"\n\n[Error: {str(e)}]"


# ============================================
# Debug/Utility Functions
# ============================================

def retrieve(query: str, top_k: int = TOP_K, use_hybrid: bool = True) -> List[Dict]:
    """
    Direct retrieve function (for debugging).

    Wrapper Ä‘Æ¡n giáº£n cá»§a retrieve_with_cache, dÃ¹ng Ä‘á»ƒ test retrieval
    Ä‘á»™c láº­p vá»›i generation.

    Args:
        query: CÃ¢u query
        top_k: Sá»‘ káº¿t quáº£
        use_hybrid: CÃ³ dÃ¹ng hybrid search khÃ´ng

    Returns:
        List[Dict]: Retrieved documents
    """
    return retrieve_with_cache(query, top_k=top_k, use_hybrid=use_hybrid)


def generate(messages: List[Dict], max_tokens: int = 1024) -> str:
    """
    Direct generate function (for debugging).

    Wrapper Ä‘Æ¡n giáº£n cá»§a generate_with_retry, dÃ¹ng Ä‘á»ƒ test generation
    Ä‘á»™c láº­p vá»›i retrieval.

    Args:
        messages: Messages cho LLM
        max_tokens: Max tokens

    Returns:
        str: Generated response
    """
    return generate_with_retry(messages, max_tokens=max_tokens)
