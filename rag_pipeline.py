"""
RAG Pipeline Module
===================
Module x·ª≠ l√Ω RAG (Retrieval-Augmented Generation) v·ªõi caching v√† retry.

RAG Pipeline Flow:
1. User g·ª≠i query
2. Detect ng√¥n ng·ªØ (vi/en)
3. Retrieve: T√¨m top-k documents li√™n quan t·ª´ vector store
4. Filter: L·ªçc documents c√≥ score >= threshold
5. Build prompt: T·∫°o prompt v·ªõi context t·ª´ retrieved docs
6. Generate: G·ªçi LLM ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi
7. Return: Tr·∫£ v·ªÅ response + sources

Caching:
- Query cache: L∆∞u k·∫øt qu·∫£ retrieval theo query hash
- FIFO eviction: Khi cache ƒë·∫ßy, x√≥a entries c≈© nh·∫•t
- Clear cache khi c√≥ document m·ªõi ƒë∆∞·ª£c ingest

Retry Logic:
- Generate c√≥ th·ªÉ fail do network/server issues
- Retry v·ªõi exponential backoff (MAX_RETRIES l·∫ßn)

Language Support:
- T·ª± ƒë·ªông detect ng√¥n ng·ªØ t·ª´ query
- System prompt v√† response message theo ng√¥n ng·ªØ

S·ª≠ d·ª•ng:
    from rag_pipeline import chat, chat_stream, retrieve

    # Chat th∆∞·ªùng
    result = chat("GPIO l√† g√¨?", top_k=5)
    print(result["response"])

    # Streaming chat
    for chunk in chat_stream("Explain I2C protocol"):
        print(chunk, end="")

    # Debug retrieval
    docs = retrieve("UART configuration")
    for d in docs:
        print(f"{d['score']:.3f}: {d['text'][:100]}...")
"""

from openai import OpenAI
from typing import List, Dict, Optional, Generator
import hashlib
import time
import logging
from datetime import datetime

from vectorstore_chroma import get_vectorstore
from config import (
    VLLM_BASE_URL, LLM_MODEL, TOP_K, RELEVANCE_THRESHOLD,
    DENSE_WEIGHT, SPARSE_WEIGHT, QUERY_CACHE_SIZE, ENABLE_QUERY_CACHE,
    MAX_RETRIES, RETRY_DELAY, TEMPERATURE, LOG_LEVEL,
    DEBUG_RETRIEVAL, DEBUG_GENERATION, DEBUG_CONTEXT
)

# Setup logging
logging.basicConfig(level=getattr(logging, LOG_LEVEL))
logger = logging.getLogger(__name__)


def log_debug(flag: bool, prefix: str, message: str):
    """
    Log debug c√≥ ƒëi·ªÅu ki·ªán.

    Args:
        flag: Debug flag t·ª´ config (DEBUG_RETRIEVAL, DEBUG_GENERATION, etc.)
        prefix: Prefix cho log (emoji + category)
        message: N·ªôi dung log
    """
    if flag:
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {prefix}: {message}")


# vLLM client - k·∫øt n·ªëi ƒë·∫øn vLLM server qua OpenAI-compatible API
llm_client = OpenAI(base_url=VLLM_BASE_URL, api_key="not-needed")

# Cache cho query embeddings
# Key: hash c·ªßa (query, top_k, use_hybrid)
# Value: List[Dict] - retrieved documents
_query_cache: Dict[str, List[Dict]] = {}
MAX_CACHE_SIZE = QUERY_CACHE_SIZE


# ============================================
# System Prompts
# ============================================

SYSTEM_PROMPT_EN = """You are an expert assistant specializing in embedded programming and embedded systems.

IMPORTANT RULES:
- ONLY answer based on the provided context from the knowledge base.
- If the context does not contain relevant information, respond EXACTLY with: "NO_RELEVANT_INFO"
- DO NOT make up or infer information not in the context.
- Always cite which document/source you got the information from.
- Provide code examples only if they exist in the context.
- For technical terms, registers, or configurations, be precise and accurate.

LANGUAGE REQUIREMENT (CRITICAL):
- You MUST respond ONLY in English.
- NEVER respond in Chinese, Japanese, Korean, or any other language.
- Even if the context contains text in other languages, your response MUST be in English."""

SYSTEM_PROMPT_VI = """B·∫°n l√† tr·ª£ l√Ω chuy√™n gia v·ªÅ l·∫≠p tr√¨nh nh√∫ng v√† h·ªá th·ªëng nh√∫ng.

QUY T·∫ÆC QUAN TR·ªåNG:
- CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p t·ª´ c∆° s·ªü ki·∫øn th·ª©c.
- N·∫øu ng·ªØ c·∫£nh KH√îNG ch·ª©a th√¥ng tin li√™n quan, tr·∫£ l·ªùi CH√çNH X√ÅC: "NO_RELEVANT_INFO"
- KH√îNG ƒê∆Ø·ª¢C b·ªãa ho·∫∑c suy lu·∫≠n th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh.
- Lu√¥n tr√≠ch d·∫´n ngu·ªìn t√†i li·ªáu m√† b·∫°n l·∫•y th√¥ng tin.
- Ch·ªâ cung c·∫•p v√≠ d·ª• code n·∫øu c√≥ trong ng·ªØ c·∫£nh.
- V·ªõi c√°c thu·∫≠t ng·ªØ k·ªπ thu·∫≠t, thanh ghi, c·∫•u h√¨nh, h√£y ch√≠nh x√°c.

Y√äU C·∫¶U NG√îN NG·ªÆ (B·∫ÆT BU·ªòC):
- B·∫°n PH·∫¢I tr·∫£ l·ªùi HO√ÄN TO√ÄN b·∫±ng ti·∫øng Vi·ªát.
- TUY·ªÜT ƒê·ªêI KH√îNG ƒë∆∞·ª£c tr·∫£ l·ªùi b·∫±ng ti·∫øng Trung, ti·∫øng Nh·∫≠t, ti·∫øng H√†n hay b·∫•t k·ª≥ ng√¥n ng·ªØ n√†o kh√°c.
- Ngay c·∫£ khi ng·ªØ c·∫£nh ch·ª©a vƒÉn b·∫£n ti·∫øng n∆∞·ªõc ngo√†i, c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n PH·∫¢I b·∫±ng ti·∫øng Vi·ªát."""


# ============================================
# Helper Functions
# ============================================

def detect_language(text: str) -> str:
    """
    Detect ng√¥n ng·ªØ c·ªßa text: Ti·∫øng Vi·ªát ho·∫∑c Ti·∫øng Anh.

    Ph∆∞∆°ng ph√°p: ƒê·∫øm s·ªë k√Ω t·ª± ti·∫øng Vi·ªát ƒë·∫∑c tr∆∞ng.
    N·∫øu c√≥ > 2 k√Ω t·ª± ti·∫øng Vi·ªát -> "vi", ng∆∞·ª£c l·∫°i -> "en"

    Args:
        text: VƒÉn b·∫£n c·∫ßn detect

    Returns:
        str: "vi" ho·∫∑c "en"

    Example:
        >>> detect_language("GPIO l√† g√¨?")
        'vi'
        >>> detect_language("What is GPIO?")
        'en'
    """
    vn_chars = set("√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë")
    text_lower = text.lower()
    vn_count = sum(1 for c in text_lower if c in vn_chars)
    return "vi" if vn_count > 2 else "en"


def contains_chinese(text: str) -> bool:
    """
    Ki·ªÉm tra xem text c√≥ ch·ª©a k√Ω t·ª± ti·∫øng Trung kh√¥ng.

    Args:
        text: VƒÉn b·∫£n c·∫ßn ki·ªÉm tra

    Returns:
        bool: True n·∫øu ch·ª©a ti·∫øng Trung (>10 k√Ω t·ª±)
    """
    chinese_count = 0
    for char in text:
        # Unicode range cho CJK characters
        if '\u4e00' <= char <= '\u9fff':
            chinese_count += 1
            if chinese_count > 10:  # Threshold: >10 k√Ω t·ª± Trung
                return True
    return False


def _cache_key(query: str, top_k: int, use_hybrid: bool) -> str:
    """
    T·∫°o cache key cho query.

    Cache key l√† MD5 hash c·ªßa (query, top_k, use_hybrid) ƒë·ªÉ:
    - ƒê·∫£m b·∫£o key ng·∫Øn g·ªçn
    - Tr√°nh special characters trong key

    Args:
        query: C√¢u query
        top_k: S·ªë k·∫øt qu·∫£
        use_hybrid: C√≥ d√πng hybrid search kh√¥ng

    Returns:
        str: MD5 hash string (32 chars)
    """
    return hashlib.md5(f"{query}:{top_k}:{use_hybrid}".encode()).hexdigest()


# ============================================
# Retrieval Functions
# ============================================

def retrieve_with_cache(
    query: str,
    top_k: int = TOP_K,
    use_hybrid: bool = True
) -> List[Dict]:
    """
    Retrieve documents v·ªõi caching.

    Qu√° tr√¨nh:
    1. T·∫°o cache key t·ª´ query params
    2. Check cache, n·∫øu hit th√¨ return cached results
    3. N·∫øu miss, g·ªçi vector store search
    4. L∆∞u k·∫øt qu·∫£ v√†o cache (v·ªõi size limit)

    Cache eviction: Simple FIFO - khi cache ƒë·∫ßy, x√≥a 25% entries c≈© nh·∫•t.

    Args:
        query: C√¢u query t√¨m ki·∫øm
        top_k: S·ªë k·∫øt qu·∫£ t·ªëi ƒëa (default t·ª´ config)
        use_hybrid: C√≥ d√πng hybrid search kh√¥ng (default: True)

    Returns:
        List[Dict]: Danh s√°ch documents t√¨m ƒë∆∞·ª£c
        M·ªói dict c√≥: id, text, score, metadata, (dense_score, sparse_score n·∫øu hybrid)
    """
    global _query_cache

    cache_key = _cache_key(query, top_k, use_hybrid)

    # Check cache
    if ENABLE_QUERY_CACHE and cache_key in _query_cache:
        log_debug(DEBUG_RETRIEVAL, "üîç RETRIEVE", f"Cache HIT for: '{query[:50]}...'")
        return _query_cache[cache_key]

    log_debug(DEBUG_RETRIEVAL, "üîç RETRIEVE", f"Cache MISS, searching: '{query[:50]}...'")

    # Retrieve t·ª´ vector store
    vs = get_vectorstore()
    results = vs.search(query, top_k=top_k, use_hybrid=use_hybrid)

    # Log top results khi DEBUG
    if DEBUG_RETRIEVAL and results:
        log_debug(DEBUG_RETRIEVAL, "üîç RETRIEVE", f"Found {len(results)} docs:")
        for i, doc in enumerate(results[:3]):  # Top 3
            source = doc.get("metadata", {}).get("source", "?")
            score = doc.get("score", 0)
            text_preview = doc.get("text", "")[:80].replace("\n", " ")
            log_debug(
                DEBUG_RETRIEVAL, "üîç RETRIEVE",
                f"  [{i+1}] {score:.3f} | {source} | {text_preview}..."
            )

    # Update cache (v·ªõi size limit)
    if ENABLE_QUERY_CACHE:
        if len(_query_cache) >= MAX_CACHE_SIZE:
            # Remove oldest entries (simple FIFO)
            keys_to_remove = list(_query_cache.keys())[:MAX_CACHE_SIZE // 4]
            for k in keys_to_remove:
                del _query_cache[k]
            log_debug(DEBUG_RETRIEVAL, "üîç RETRIEVE", f"Cache eviction: removed {len(keys_to_remove)} entries")

        _query_cache[cache_key] = results

    return results


def clear_cache():
    """
    Clear query cache.

    G·ªçi khi:
    - C√≥ document m·ªõi ƒë∆∞·ª£c ingest
    - Document b·ªã x√≥a
    - User y√™u c·∫ßu clear cache

    Side effects:
    - Reset _query_cache v·ªÅ empty dict
    """
    global _query_cache
    _query_cache = {}
    logger.info("Query cache cleared")
    log_debug(DEBUG_RETRIEVAL, "üîç RETRIEVE", "Cache cleared")


# ============================================
# Context Formatting
# ============================================

def format_context(docs: List[Dict]) -> tuple:
    """
    Format retrieved docs th√†nh context string cho LLM.

    Qu√° tr√¨nh:
    1. L·ªçc docs c√≥ score >= RELEVANCE_THRESHOLD
    2. Format m·ªói doc v·ªõi source, score, text
    3. Join t·∫•t c·∫£ docs v·ªõi separator

    Args:
        docs: List documents t·ª´ retrieval

    Returns:
        tuple: (context_string, has_relevant_docs, relevant_docs_list)
        - context_string: Formatted context ho·∫∑c "No relevant documents found."
        - has_relevant_docs: True n·∫øu c√≥ √≠t nh·∫•t 1 doc relevant
        - relevant_docs_list: List c√°c docs ƒë√£ l·ªçc

    Example output:
        [1] Source: gpio.pdf (chunk 5, relevance: 0.85)
        GPIO (General Purpose Input/Output) l√†...

        ---

        [2] Source: gpio.pdf (chunk 6, relevance: 0.82)
        ƒê·ªÉ c·∫•u h√¨nh GPIO mode...
    """
    if not docs:
        return "No relevant documents found.", False, []

    # Filter theo relevance threshold
    relevant_docs = [d for d in docs if d.get("score", 0) >= RELEVANCE_THRESHOLD]

    if not relevant_docs:
        scores_str = [f"{d.get('score', 0):.3f}" for d in docs[:5]]
        log_debug(
            DEBUG_CONTEXT, "üìã CONTEXT",
            f"No docs above threshold {RELEVANCE_THRESHOLD}, all scores: {scores_str}"
        )
        return "No relevant documents found.", False, []

    log_debug(
        DEBUG_CONTEXT, "üìã CONTEXT",
        f"Filtered {len(relevant_docs)}/{len(docs)} docs (threshold={RELEVANCE_THRESHOLD})"
    )

    # Format t·ª´ng doc
    context_parts = []
    for i, doc in enumerate(relevant_docs, 1):
        source = doc.get("metadata", {}).get("source", "Unknown")
        score = doc.get("score", 0)
        chunk_idx = doc.get("metadata", {}).get("chunk_index", "?")
        text = doc.get("text", "")

        # Th√™m th√¥ng tin v·ªÅ lo·∫°i score n·∫øu c√≥ (hybrid search)
        score_info = f"relevance: {score:.2f}"
        if "dense_score" in doc and "sparse_score" in doc:
            score_info += f", dense: {doc['dense_score']:.2f}, sparse: {doc['sparse_score']:.2f}"

        context_parts.append(
            f"[{i}] Source: {source} (chunk {chunk_idx}, {score_info})\n{text}"
        )

    context_str = "\n\n---\n\n".join(context_parts)

    # Log context khi DEBUG_CONTEXT
    if DEBUG_CONTEXT:
        log_debug(DEBUG_CONTEXT, "üìã CONTEXT", f"Context length: {len(context_str)} chars")
        log_debug(DEBUG_CONTEXT, "üìã CONTEXT", f"Context preview:\n{context_str[:500]}...")

    return context_str, True, relevant_docs


# ============================================
# Prompt Building
# ============================================

def build_prompt(query: str, context: str, lang: str) -> List[Dict]:
    """
    Build prompt messages cho LLM.

    C·∫•u tr√∫c messages:
    1. System message: H∆∞·ªõng d·∫´n role v√† rules
    2. User message: Context + Question

    Args:
        query: C√¢u h·ªèi c·ªßa user
        context: Context ƒë√£ format t·ª´ retrieved docs
        lang: Ng√¥n ng·ªØ ("vi" ho·∫∑c "en")

    Returns:
        List[Dict]: Messages cho OpenAI-compatible API
        [{"role": "system", "content": ...}, {"role": "user", "content": ...}]
    """
    system = SYSTEM_PROMPT_VI if lang == "vi" else SYSTEM_PROMPT_EN

    if lang == "vi":
        user_content = f"""Ng·ªØ c·∫£nh t·ª´ c∆° s·ªü ki·∫øn th·ª©c:
{context}

---
C√¢u h·ªèi: {query}

H√£y tr·∫£ l·ªùi chi ti·∫øt d·ª±a tr√™n ng·ªØ c·∫£nh. N·∫øu kh√¥ng c√≥ th√¥ng tin li√™n quan, tr·∫£ l·ªùi "NO_RELEVANT_INFO"."""
    else:
        user_content = f"""Context from knowledge base:
{context}

---
Question: {query}

Provide a detailed answer based on the context. If no relevant information, respond with "NO_RELEVANT_INFO"."""

    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": user_content}
    ]

    # Log prompt khi DEBUG_GENERATION
    if DEBUG_GENERATION:
        log_debug(DEBUG_GENERATION, "‚ö° GENERATE", f"Prompt length: {len(user_content)} chars")
        log_debug(DEBUG_GENERATION, "‚ö° GENERATE", f"System prompt: {system[:100]}...")

    return messages


# ============================================
# Generation Functions
# ============================================

def generate_with_retry(
    messages: List[Dict],
    max_tokens: int = 1024,
    temperature: float = TEMPERATURE
) -> str:
    """
    Generate response t·ª´ LLM v·ªõi retry logic.

    Qu√° tr√¨nh:
    1. G·ªçi vLLM qua OpenAI-compatible API
    2. N·∫øu fail, retry v·ªõi exponential backoff
    3. Sau MAX_RETRIES l·∫ßn fail, raise exception

    Retry delays: RETRY_DELAY * attempt (1s, 2s, 3s, ...)

    Args:
        messages: List messages (system + user)
        max_tokens: S·ªë tokens t·ªëi ƒëa cho response (default: 1024)
        temperature: Sampling temperature (default t·ª´ config)

    Returns:
        str: Generated response t·ª´ LLM

    Raises:
        Exception: N·∫øu fail sau MAX_RETRIES l·∫ßn

    Example:
        >>> messages = [{"role": "user", "content": "Hello"}]
        >>> response = generate_with_retry(messages, max_tokens=100)
    """
    last_error = None

    log_debug(
        DEBUG_GENERATION, "‚ö° GENERATE",
        f"Calling LLM: model={LLM_MODEL}, max_tokens={max_tokens}, temp={temperature}"
    )

    for attempt in range(MAX_RETRIES):
        try:
            start_time = time.time()

            response = llm_client.chat.completions.create(
                model=LLM_MODEL,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9
            )

            result = response.choices[0].message.content
            elapsed = time.time() - start_time

            log_debug(
                DEBUG_GENERATION, "‚ö° GENERATE",
                f"Response received: {len(result)} chars in {elapsed:.2f}s"
            )

            # Ki·ªÉm tra n·∫øu response ch·ª©a ti·∫øng Trung -> retry v·ªõi prompt m·∫°nh h∆°n
            if contains_chinese(result):
                log_debug(
                    DEBUG_GENERATION, "‚ö° GENERATE",
                    "‚ö†Ô∏è Response contains Chinese! Retrying with stronger prompt..."
                )
                # Th√™m instruction m·∫°nh h∆°n v√†o messages
                retry_messages = messages.copy()
                retry_messages.append({
                    "role": "assistant",
                    "content": result[:100]  # Partial response
                })
                retry_messages.append({
                    "role": "user",
                    "content": "STOP! You responded in Chinese which is FORBIDDEN. Please respond ONLY in Vietnamese or English. Rewrite your answer:"
                })

                retry_response = llm_client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=retry_messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=0.9
                )
                result = retry_response.choices[0].message.content

                # N·∫øu v·∫´n ti·∫øng Trung, tr·∫£ v·ªÅ message l·ªói
                if contains_chinese(result):
                    log_debug(
                        DEBUG_GENERATION, "‚ö° GENERATE",
                        "‚ö†Ô∏è Still Chinese after retry, returning error message"
                    )
                    return "Xin l·ªói, h·ªá th·ªëng g·∫∑p l·ªói ng√¥n ng·ªØ. Vui l√≤ng th·ª≠ l·∫°i c√¢u h·ªèi."

            # Log response preview
            if DEBUG_GENERATION:
                log_debug(
                    DEBUG_GENERATION, "‚ö° GENERATE",
                    f"Response preview: {result[:200]}..."
                )

            return result

        except Exception as e:
            last_error = e
            logger.warning(f"Generation attempt {attempt + 1} failed: {e}")
            log_debug(
                DEBUG_GENERATION, "‚ö° GENERATE",
                f"Attempt {attempt + 1} failed: {e}"
            )

            if attempt < MAX_RETRIES - 1:
                sleep_time = RETRY_DELAY * (attempt + 1)
                log_debug(DEBUG_GENERATION, "‚ö° GENERATE", f"Retrying in {sleep_time}s...")
                time.sleep(sleep_time)

    logger.error(f"Generation failed after {MAX_RETRIES} attempts: {last_error}")
    raise last_error


# ============================================
# Main Chat Functions
# ============================================

def chat(
    query: str,
    top_k: int = TOP_K,
    max_tokens: int = 1024,
    use_hybrid: bool = True
) -> Dict:
    """
    Main chat function - X·ª≠ l√Ω query v√† tr·∫£ v·ªÅ response.

    RAG Pipeline:
    1. Detect ng√¥n ng·ªØ
    2. Retrieve top-k docs t·ª´ vector store
    3. Filter docs theo relevance threshold
    4. Build prompt v·ªõi context
    5. Generate response t·ª´ LLM
    6. Return k·∫øt qu·∫£ v·ªõi metadata

    Args:
        query: C√¢u h·ªèi c·ªßa user
        top_k: S·ªë docs retrieve (default t·ª´ config)
        max_tokens: Max tokens cho response (default: 1024)
        use_hybrid: C√≥ d√πng hybrid search kh√¥ng (default: True)

    Returns:
        Dict v·ªõi c√°c keys:
        - query: C√¢u query g·ªëc
        - response: C√¢u tr·∫£ l·ªùi t·ª´ LLM
        - language: "vi" ho·∫∑c "en"
        - sources: List sources ƒë∆∞·ª£c s·ª≠ d·ª•ng
        - context_used: True n·∫øu c√≥ s·ª≠ d·ª•ng context
        - retrieval_info: Dict v·ªõi timing v√† stats:
            - docs_found: S·ªë docs t√¨m ƒë∆∞·ª£c
            - docs_relevant: S·ªë docs v∆∞·ª£t threshold
            - retrieve_time_ms: Th·ªùi gian retrieve (ms)
            - generate_time_ms: Th·ªùi gian generate (ms)
            - total_time_ms: T·ªïng th·ªùi gian (ms)
            - hybrid_search: C√≥ d√πng hybrid kh√¥ng

    Example:
        >>> result = chat("GPIO l√† g√¨?")
        >>> print(result["response"])
        GPIO (General Purpose Input/Output) l√† c√°c ch√¢n ƒëa nƒÉng...
        >>> print(result["sources"])
        [{"source": "stm32.pdf", "score": 0.85, "chunk_index": 5}]
    """
    start_time = time.time()
    lang = detect_language(query)

    log_debug(DEBUG_GENERATION, "‚ö° CHAT", f"Query: '{query[:80]}...', lang={lang}")

    # Retrieve
    retrieve_start = time.time()
    retrieved_docs = retrieve_with_cache(query, top_k=top_k, use_hybrid=use_hybrid)
    retrieve_time = time.time() - retrieve_start

    log_debug(
        DEBUG_RETRIEVAL, "üîç RETRIEVE",
        f"Retrieved {len(retrieved_docs)} docs in {retrieve_time*1000:.0f}ms"
    )

    # Format context
    context, has_relevant, relevant_docs = format_context(retrieved_docs)

    # No info message theo ng√¥n ng·ªØ
    no_info_msg = (
        "T√¥i kh√¥ng c√≥ th√¥ng tin v·ªÅ ch·ªß ƒë·ªÅ n√†y trong t√†i li·ªáu hi·ªán t·∫°i. "
        "Vui l√≤ng upload t√†i li·ªáu li√™n quan ho·∫∑c h·ªèi c√¢u h·ªèi kh√°c."
    ) if lang == "vi" else (
        "I don't have information about this topic in the current documents. "
        "Please upload relevant documents or ask another question."
    )

    # N·∫øu kh√¥ng c√≥ context relevant
    if not has_relevant:
        log_debug(DEBUG_GENERATION, "‚ö° CHAT", "No relevant context, returning no_info message")
        return {
            "query": query,
            "response": no_info_msg,
            "language": lang,
            "sources": [],
            "context_used": False,
            "retrieval_info": {
                "docs_found": len(retrieved_docs),
                "docs_relevant": 0,
                "retrieve_time_ms": int(retrieve_time * 1000),
                "hybrid_search": use_hybrid
            }
        }

    # Generate
    generate_start = time.time()
    messages = build_prompt(query, context, lang)
    response = generate_with_retry(messages, max_tokens=max_tokens)
    generate_time = time.time() - generate_start

    log_debug(
        DEBUG_GENERATION, "‚ö° GENERATE",
        f"Generated response in {generate_time*1000:.0f}ms"
    )

    # Check if model says no info
    if "NO_RELEVANT_INFO" in response:
        log_debug(DEBUG_GENERATION, "‚ö° CHAT", "LLM returned NO_RELEVANT_INFO")
        return {
            "query": query,
            "response": no_info_msg,
            "language": lang,
            "sources": [],
            "context_used": False,
            "retrieval_info": {
                "docs_found": len(retrieved_docs),
                "docs_relevant": len(relevant_docs),
                "retrieve_time_ms": int(retrieve_time * 1000),
                "generate_time_ms": int(generate_time * 1000),
                "hybrid_search": use_hybrid
            }
        }

    # Build sources list
    sources = [{
        "source": d.get("metadata", {}).get("source"),
        "score": round(d.get("score", 0), 3),
        "chunk_index": d.get("metadata", {}).get("chunk_index")
    } for d in relevant_docs]

    total_time = time.time() - start_time

    log_debug(
        DEBUG_GENERATION, "‚ö° CHAT",
        f"Chat completed: {total_time*1000:.0f}ms total, "
        f"{len(sources)} sources used"
    )

    return {
        "query": query,
        "response": response,
        "language": lang,
        "sources": sources,
        "context_used": True,
        "retrieval_info": {
            "docs_found": len(retrieved_docs),
            "docs_relevant": len(relevant_docs),
            "retrieve_time_ms": int(retrieve_time * 1000),
            "generate_time_ms": int(generate_time * 1000),
            "total_time_ms": int(total_time * 1000),
            "hybrid_search": use_hybrid
        }
    }


def chat_stream(
    query: str,
    top_k: int = TOP_K,
    max_tokens: int = 1024,
    use_hybrid: bool = True
) -> Generator[str, None, None]:
    """
    Streaming chat - Tr·∫£ v·ªÅ response t·ª´ng chunk.

    T∆∞∆°ng t·ª± chat() nh∆∞ng yield t·ª´ng token thay v√¨ tr·∫£ v·ªÅ to√†n b·ªô.
    D√πng cho real-time display trong UI.

    Args:
        query: C√¢u h·ªèi c·ªßa user
        top_k: S·ªë docs retrieve
        max_tokens: Max tokens cho response
        use_hybrid: C√≥ d√πng hybrid search kh√¥ng

    Yields:
        str: T·ª´ng chunk c·ªßa response

    Example:
        >>> for chunk in chat_stream("GPIO l√† g√¨?"):
        ...     print(chunk, end="", flush=True)
    """
    lang = detect_language(query)

    log_debug(DEBUG_GENERATION, "‚ö° STREAM", f"Stream query: '{query[:50]}...'")

    # Retrieve
    retrieved_docs = retrieve_with_cache(query, top_k=top_k, use_hybrid=use_hybrid)
    context, has_relevant, relevant_docs = format_context(retrieved_docs)

    if not has_relevant:
        no_info_msg = (
            "T√¥i kh√¥ng c√≥ th√¥ng tin v·ªÅ ch·ªß ƒë·ªÅ n√†y trong t√†i li·ªáu hi·ªán t·∫°i. "
            "Vui l√≤ng upload t√†i li·ªáu li√™n quan ho·∫∑c h·ªèi c√¢u h·ªèi kh√°c."
        ) if lang == "vi" else (
            "I don't have information about this topic in the current documents. "
            "Please upload relevant documents or ask another question."
        )
        yield no_info_msg
        return

    messages = build_prompt(query, context, lang)

    try:
        log_debug(DEBUG_GENERATION, "‚ö° STREAM", "Starting stream generation...")

        response = llm_client.chat.completions.create(
            model=LLM_MODEL,
            messages=messages,
            max_tokens=max_tokens,
            temperature=TEMPERATURE,
            stream=True
        )

        for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

        log_debug(DEBUG_GENERATION, "‚ö° STREAM", "Stream completed")

    except Exception as e:
        logger.error(f"Stream generation error: {e}")
        log_debug(DEBUG_GENERATION, "‚ö° STREAM", f"Stream error: {e}")
        yield f"\n\n[Error: {str(e)}]"


# ============================================
# Debug/Utility Functions
# ============================================

def retrieve(query: str, top_k: int = TOP_K, use_hybrid: bool = True) -> List[Dict]:
    """
    Direct retrieve function (for debugging).

    Wrapper ƒë∆°n gi·∫£n c·ªßa retrieve_with_cache, d√πng ƒë·ªÉ test retrieval
    ƒë·ªôc l·∫≠p v·ªõi generation.

    Args:
        query: C√¢u query
        top_k: S·ªë k·∫øt qu·∫£
        use_hybrid: C√≥ d√πng hybrid search kh√¥ng

    Returns:
        List[Dict]: Retrieved documents
    """
    return retrieve_with_cache(query, top_k=top_k, use_hybrid=use_hybrid)


def generate(messages: List[Dict], max_tokens: int = 1024) -> str:
    """
    Direct generate function (for debugging).

    Wrapper ƒë∆°n gi·∫£n c·ªßa generate_with_retry, d√πng ƒë·ªÉ test generation
    ƒë·ªôc l·∫≠p v·ªõi retrieval.

    Args:
        messages: Messages cho LLM
        max_tokens: Max tokens

    Returns:
        str: Generated response
    """
    return generate_with_retry(messages, max_tokens=max_tokens)
