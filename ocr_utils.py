"""
OCR vÃ  Vision Captioning module
================================
Module nÃ y cung cáº¥p kháº£ nÄƒng trÃ­ch xuáº¥t vÄƒn báº£n tá»« hÃ¬nh áº£nh (OCR)
vÃ  táº¡o mÃ´ táº£ ngá»¯ nghÄ©a cho hÃ¬nh áº£nh ká»¹ thuáº­t (Vision Captioning).

CÃ¡c thÃ nh pháº§n chÃ­nh:
- OCREngine: Sá»­ dá»¥ng PaddleOCR Ä‘á»ƒ nháº­n dáº¡ng vÄƒn báº£n tiáº¿ng Viá»‡t/Anh
- VisionCaptioner: Sá»­ dá»¥ng Qwen2-VL Ä‘á»ƒ mÃ´ táº£ ná»™i dung hÃ¬nh áº£nh
- process_image(): Káº¿t há»£p cáº£ OCR vÃ  Vision Ä‘á»ƒ trÃ­ch xuáº¥t thÃ´ng tin Ä‘áº§y Ä‘á»§

Sá»­ dá»¥ng:
    from ocr_utils import process_image
    result = process_image("path/to/image.png", lang="vi")
    # result = {"ocr_text": "...", "caption": "...", "combined": "..."}
"""

from paddleocr import PaddleOCR
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from PIL import Image
import torch
from typing import Dict, List
from datetime import datetime

from config import VISION_MODEL, DEBUG_OCR, DEBUG_VISION


def log_debug(flag: bool, prefix: str, message: str):
    """
    HÃ m helper Ä‘á»ƒ log debug cÃ³ Ä‘iá»u kiá»‡n.

    Args:
        flag: Cá» debug (True Ä‘á»ƒ hiá»ƒn thá»‹ log)
        prefix: Tiá»n tá»‘ cho log (vÃ­ dá»¥: "ðŸ” OCR")
        message: Ná»™i dung log
    """
    if flag:
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {prefix}: {message}")


class OCREngine:
    """
    PaddleOCR Engine - Nháº­n dáº¡ng vÄƒn báº£n tá»« hÃ¬nh áº£nh.

    Sá»­ dá»¥ng PaddleOCR 3.x vá»›i há»— trá»£ tiáº¿ng Viá»‡t Ä‘á»ƒ trÃ­ch xuáº¥t
    vÄƒn báº£n tá»« hÃ¬nh áº£nh nhÆ° sÆ¡ Ä‘á»“ máº¡ch, báº£ng thanh ghi, chÃº thÃ­ch.

    Singleton Pattern: Chá»‰ táº¡o má»™t instance duy nháº¥t Ä‘á»ƒ tiáº¿t kiá»‡m bá»™ nhá»›.

    Attributes:
        reader: PaddleOCR instance

    Example:
        engine = OCREngine()
        text = engine.extract_text("diagram.png")
    """
    _instance = None

    def __new__(cls):
        """Singleton pattern - Ä‘áº£m báº£o chá»‰ cÃ³ 1 instance."""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._init_ocr()
        return cls._instance

    def _init_ocr(self):
        """
        Khá»Ÿi táº¡o PaddleOCR engine.

        PaddleOCR 3.x tá»± Ä‘á»™ng detect GPU vÃ  sá»­ dá»¥ng cÃ¡c model
        PP-OCRv5 má»›i nháº¥t cho tiáº¿ng Viá»‡t.
        """
        print("Initializing PaddleOCR...")
        try:
            # PaddleOCR 3.x: chá»‰ cáº§n lang, cÃ¡c parameter khÃ¡c Ä‘Ã£ deprecated
            self.reader = PaddleOCR(lang='vi')
            print("PaddleOCR ready")
        except Exception as e:
            print(f"PaddleOCR init error: {e}")
            self.reader = None

    def extract_text(self, image_path: str) -> str:
        """
        TrÃ­ch xuáº¥t vÄƒn báº£n tá»« hÃ¬nh áº£nh.

        Quy trÃ¬nh:
        1. Gá»i PaddleOCR predict() Ä‘á»ƒ phÃ¡t hiá»‡n vÃ  nháº­n dáº¡ng text
        2. Lá»c cÃ¡c káº¿t quáº£ cÃ³ confidence > 0.5
        3. GhÃ©p cÃ¡c dÃ²ng text thÃ nh chuá»—i

        Args:
            image_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file hÃ¬nh áº£nh

        Returns:
            str: VÄƒn báº£n Ä‘Æ°á»£c trÃ­ch xuáº¥t, cÃ¡c dÃ²ng cÃ¡ch nhau bá»Ÿi newline.
                 Tráº£ vá» chuá»—i rá»—ng náº¿u khÃ´ng tÃ¬m tháº¥y text hoáº·c cÃ³ lá»—i.

        Example:
            text = engine.extract_text("circuit.png")
            # "VCC\nGND\nPin 1: TX\nPin 2: RX"
        """
        try:
            if self.reader is None:
                print("OCR reader not initialized")
                return ""

            log_debug(DEBUG_OCR, "ðŸ” OCR", f"Processing: {image_path}")

            # PaddleOCR 3.x: sá»­ dá»¥ng predict() thay vÃ¬ ocr()
            result = self.reader.predict(image_path)
            if not result:
                log_debug(DEBUG_OCR, "ðŸ” OCR", "No text detected")
                return ""

            lines = []
            # PaddleOCR 3.x tráº£ vá» list of dicts hoáº·c list of lists
            for item in result:
                if isinstance(item, dict):
                    # Format má»›i: {'rec_texts': [...], 'rec_scores': [...]}
                    texts = item.get('rec_texts', [])
                    scores = item.get('rec_scores', [])
                    for text, score in zip(texts, scores):
                        if score > 0.5:
                            lines.append(text)
                            log_debug(DEBUG_OCR, "ðŸ” OCR", f"  [{score:.2f}] {text[:50]}...")
                elif isinstance(item, list):
                    # Format cÅ©: [[box, (text, conf)], ...]
                    for line in item:
                        if len(line) >= 2 and isinstance(line[1], tuple):
                            text = line[1][0]
                            conf = line[1][1]
                            if conf > 0.5:
                                lines.append(text)
                                log_debug(DEBUG_OCR, "ðŸ” OCR", f"  [{conf:.2f}] {text[:50]}...")

            result_text = "\n".join(lines)
            log_debug(DEBUG_OCR, "ðŸ” OCR", f"Extracted {len(lines)} lines, {len(result_text)} chars")

            return result_text
        except Exception as e:
            print(f"OCR error: {e}")
            return ""


class VisionCaptioner:
    """
    Vision Captioner - Táº¡o mÃ´ táº£ ngá»¯ nghÄ©a cho hÃ¬nh áº£nh ká»¹ thuáº­t.

    Sá»­ dá»¥ng Qwen2-VL-7B Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  mÃ´ táº£ ná»™i dung hÃ¬nh áº£nh
    nhÆ° sÆ¡ Ä‘á»“ máº¡ch, timing diagram, flowchart, biá»ƒu Ä‘á»“.

    Model Ä‘Æ°á»£c giá»¯ trong memory sau láº§n load Ä‘áº§u tiÃªn Ä‘á»ƒ tÄƒng tá»‘c
    cÃ¡c láº§n xá»­ lÃ½ tiáº¿p theo.

    Singleton Pattern: Chá»‰ táº¡o má»™t instance duy nháº¥t.

    Attributes:
        _model: Qwen2-VL model instance
        _processor: Qwen2-VL processor Ä‘á»ƒ xá»­ lÃ½ input
        _loaded: Flag cho biáº¿t model Ä‘Ã£ Ä‘Æ°á»£c load chÆ°a
        _disabled: Flag Ä‘á»ƒ disable khi khÃ´ng Ä‘á»§ VRAM

    Example:
        captioner = VisionCaptioner()
        caption = captioner.caption_image("schematic.png", lang="vi")
    """
    _instance = None
    _model = None
    _processor = None
    _loaded = False
    _disabled = False  # Disable khi khÃ´ng Ä‘á»§ VRAM

    def __new__(cls):
        """Singleton pattern."""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def _check_vram(self, required_gb: float = 10.0) -> bool:
        """
        Kiá»ƒm tra VRAM cÃ²n trá»‘ng cÃ³ Ä‘á»§ Ä‘á»ƒ load model khÃ´ng.

        Args:
            required_gb: Sá»‘ GB VRAM cáº§n thiáº¿t (máº·c Ä‘á»‹nh 10GB)

        Returns:
            bool: True náº¿u Ä‘á»§ VRAM, False náº¿u khÃ´ng Ä‘á»§
        """
        try:
            if torch.cuda.is_available():
                free_memory = torch.cuda.mem_get_info()[0] / (1024**3)  # Convert to GB
                print(f"Available VRAM: {free_memory:.2f} GB, required: {required_gb} GB")
                return free_memory >= required_gb
            return False
        except:
            return False

    def load_model(self) -> bool:
        """
        Load Vision model vÃ o memory.

        Quy trÃ¬nh:
        1. Kiá»ƒm tra náº¿u Ä‘Ã£ load -> return True
        2. Kiá»ƒm tra náº¿u Ä‘Ã£ disable (khÃ´ng Ä‘á»§ VRAM) -> return False
        3. Kiá»ƒm tra VRAM cÃ²n trá»‘ng (cáº§n ~16GB)
        4. Load model tá»« Hugging Face vá»›i float16 precision
        5. Load processor Ä‘á»ƒ xá»­ lÃ½ input

        Returns:
            bool: True náº¿u load thÃ nh cÃ´ng, False náº¿u tháº¥t báº¡i
        """
        if self._loaded:
            return True

        if self._disabled:
            return False

        # Kiá»ƒm tra VRAM (cáº§n ~16GB cho Qwen2-VL-7B vá»›i float16)
        if not self._check_vram(16.0):
            print("WARNING: Not enough VRAM for Vision model. Skipping image captioning.")
            self._disabled = True
            return False

        try:
            print(f"Loading Vision model: {VISION_MODEL}")
            log_debug(DEBUG_VISION, "ðŸ–¼ï¸ Vision", "Loading Qwen2-VL-7B model...")

            self._model = Qwen2VLForConditionalGeneration.from_pretrained(
                VISION_MODEL,
                torch_dtype=torch.float16,  # Sá»­ dá»¥ng FP16 Ä‘á»ƒ tiáº¿t kiá»‡m VRAM
                device_map="auto",          # Tá»± Ä‘á»™ng map lÃªn GPU
                trust_remote_code=True
            )
            self._processor = AutoProcessor.from_pretrained(
                VISION_MODEL,
                trust_remote_code=True
            )
            self._loaded = True
            print("Vision model loaded and kept in memory")
            log_debug(DEBUG_VISION, "ðŸ–¼ï¸ Vision", "Model ready")
            return True
        except Exception as e:
            print(f"Failed to load Vision model: {e}")
            self._disabled = True
            return False

    def caption_image(self, image_path: str, lang: str = "en") -> str:
        """
        Táº¡o mÃ´ táº£ ngá»¯ nghÄ©a cho hÃ¬nh áº£nh ká»¹ thuáº­t.

        Quy trÃ¬nh:
        1. Load model náº¿u chÆ°a load
        2. Chá»n prompt phÃ¹ há»£p theo ngÃ´n ngá»¯ (vi/en)
        3. Má»Ÿ vÃ  convert áº£nh sang RGB
        4. Chuáº©n bá»‹ input vá»›i chat template
        5. Generate caption vá»›i model
        6. Giáº£i phÃ³ng memory Ä‘á»ƒ trÃ¡nh fragmentation

        Args:
            image_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file hÃ¬nh áº£nh
            lang: NgÃ´n ngá»¯ output ("vi" hoáº·c "en")

        Returns:
            str: MÃ´ táº£ ngá»¯ nghÄ©a cá»§a hÃ¬nh áº£nh.
                 Tráº£ vá» chuá»—i rá»—ng náº¿u khÃ´ng load Ä‘Æ°á»£c model hoáº·c cÃ³ lá»—i.

        Example:
            caption = captioner.caption_image("circuit.png", lang="vi")
            # "SÆ¡ Ä‘á»“ máº¡ch Ä‘iá»u khiá»ƒn LED sá»­ dá»¥ng transistor NPN..."
        """
        try:
            if not self.load_model():
                return ""  # Skip náº¿u khÃ´ng load Ä‘Æ°á»£c model

            log_debug(DEBUG_VISION, "ðŸ–¼ï¸ Vision", f"Processing: {image_path}")

            # Prompt Ä‘Æ°á»£c thiáº¿t káº¿ cho tÃ i liá»‡u ká»¹ thuáº­t embedded
            if lang == "vi":
                prompt = "MÃ´ táº£ chi tiáº¿t hÃ¬nh áº£nh ká»¹ thuáº­t nÃ y, táº­p trung vÃ o sÆ¡ Ä‘á»“ máº¡ch, code, linh kiá»‡n, cáº¥u hÃ¬nh chÃ¢n, hoáº·c thÃ´ng tin há»‡ thá»‘ng nhÃºng."
            else:
                prompt = "Describe this technical image in detail, focusing on circuit diagrams, code, hardware components, pin configurations, or embedded systems information."

            # Má»Ÿ áº£nh vÃ  convert sang RGB
            image = Image.open(image_path).convert("RGB")

            # Chuáº©n bá»‹ input theo format chat cá»§a Qwen2-VL
            messages = [{
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }]

            # Apply chat template
            text = self._processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )

            # Tokenize vÃ  chuyá»ƒn lÃªn GPU
            inputs = self._processor(
                text=[text], images=[image], padding=True, return_tensors="pt"
            ).to(self._model.device)

            # Generate caption (khÃ´ng dÃ¹ng sampling Ä‘á»ƒ cÃ³ káº¿t quáº£ nháº¥t quÃ¡n)
            with torch.no_grad():
                output_ids = self._model.generate(
                    **inputs,
                    max_new_tokens=256,  # Giá»›i háº¡n Ä‘á»™ dÃ i output
                    do_sample=False      # Deterministic output
                )

            # Decode output (bá» pháº§n input)
            output_ids = output_ids[:, inputs.input_ids.shape[1]:]
            caption = self._processor.batch_decode(
                output_ids, skip_special_tokens=True
            )[0]

            # Giáº£i phÃ³ng memory sau má»—i caption Ä‘á»ƒ trÃ¡nh fragmentation
            del inputs, output_ids
            torch.cuda.empty_cache()

            log_debug(DEBUG_VISION, "ðŸ–¼ï¸ Vision", f"Caption ({len(caption)} chars): {caption[:100]}...")

            return caption
        except Exception as e:
            print(f"Vision error: {e}")
            return ""

    def caption_batch(self, image_paths: List[str], lang: str = "en") -> List[str]:
        """
        Táº¡o caption cho nhiá»u áº£nh (xá»­ lÃ½ tuáº§n tá»±).

        Args:
            image_paths: Danh sÃ¡ch Ä‘Æ°á»ng dáº«n Ä‘áº¿n cÃ¡c file áº£nh
            lang: NgÃ´n ngá»¯ output ("vi" hoáº·c "en")

        Returns:
            List[str]: Danh sÃ¡ch caption tÆ°Æ¡ng á»©ng vá»›i tá»«ng áº£nh
        """
        captions = []
        for i, path in enumerate(image_paths):
            log_debug(DEBUG_VISION, "ðŸ–¼ï¸ Vision", f"Batch [{i+1}/{len(image_paths)}]: {path}")
            caption = self.caption_image(path, lang)
            captions.append(caption)
        return captions

    def unload_model(self):
        """
        Giáº£i phÃ³ng VRAM báº±ng cÃ¡ch unload model.

        Sá»­ dá»¥ng khi cáº§n giáº£i phÃ³ng VRAM cho cÃ¡c tÃ¡c vá»¥ khÃ¡c.
        Sau khi unload, cáº§n gá»i load_model() láº¡i Ä‘á»ƒ sá»­ dá»¥ng.
        """
        if self._loaded:
            del self._model
            del self._processor
            self._model = None
            self._processor = None
            self._loaded = False
            torch.cuda.empty_cache()
            print("Vision model unloaded")


def process_image(image_path: str, lang: str = "en") -> Dict:
    """
    Xá»­ lÃ½ hÃ¬nh áº£nh: káº¿t há»£p OCR vÃ  Vision captioning.

    ÄÃ¢y lÃ  hÃ m chÃ­nh Ä‘á»ƒ trÃ­ch xuáº¥t thÃ´ng tin tá»« hÃ¬nh áº£nh. NÃ³ káº¿t há»£p:
    - OCR: TrÃ­ch xuáº¥t vÄƒn báº£n cÃ³ trong áº£nh (chÃº thÃ­ch, giÃ¡ trá»‹, label)
    - Vision: Táº¡o mÃ´ táº£ ngá»¯ nghÄ©a vá» cáº¥u trÃºc vÃ  Ã½ nghÄ©a hÃ¬nh áº£nh

    Quy trÃ¬nh:
    1. Gá»i OCREngine Ä‘á»ƒ trÃ­ch xuáº¥t text
    2. Gá»i VisionCaptioner Ä‘á»ƒ táº¡o mÃ´ táº£
    3. Káº¿t há»£p káº¿t quáº£ thÃ nh format thá»‘ng nháº¥t

    Args:
        image_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file hÃ¬nh áº£nh
        lang: NgÃ´n ngá»¯ cho Vision caption ("vi" hoáº·c "en")

    Returns:
        Dict vá»›i cÃ¡c key:
        - ocr_text: VÄƒn báº£n tá»« OCR
        - caption: MÃ´ táº£ tá»« Vision model
        - combined: Káº¿t há»£p cáº£ hai theo format:
            [Image Description]
            {caption}

            [OCR Text]
            {ocr_text}

    Example:
        result = process_image("diagram.png", lang="vi")
        print(result["combined"])
    """
    log_debug(DEBUG_OCR or DEBUG_VISION, "ðŸ“· Image", f"Processing: {image_path}")

    # BÆ°á»›c 1: OCR - TrÃ­ch xuáº¥t vÄƒn báº£n
    ocr_engine = OCREngine()
    ocr_text = ocr_engine.extract_text(image_path)

    # BÆ°á»›c 2: Vision - Táº¡o mÃ´ táº£ ngá»¯ nghÄ©a
    captioner = VisionCaptioner()
    caption = captioner.caption_image(image_path, lang)
    # KhÃ´ng unload model ná»¯a - giá»¯ trong memory Ä‘á»ƒ xá»­ lÃ½ nhanh hÆ¡n

    # BÆ°á»›c 3: Káº¿t há»£p káº¿t quáº£
    if ocr_text and caption:
        combined = f"[Image Description]\n{caption}\n\n[OCR Text]\n{ocr_text}"
    elif caption:
        combined = f"[Image Description]\n{caption}"
    elif ocr_text:
        combined = f"[OCR Text]\n{ocr_text}"
    else:
        combined = "[No content extracted from image]"

    log_debug(DEBUG_OCR or DEBUG_VISION, "ðŸ“· Image",
              f"Result: OCR={len(ocr_text)} chars, Caption={len(caption)} chars")

    return {
        "ocr_text": ocr_text,
        "caption": caption,
        "combined": combined
    }


def get_ocr_engine() -> OCREngine:
    """
    Factory function Ä‘á»ƒ láº¥y OCREngine instance.

    Returns:
        OCREngine: Singleton instance cá»§a OCREngine
    """
    return OCREngine()


def get_vision_captioner() -> VisionCaptioner:
    """
    Factory function Ä‘á»ƒ láº¥y VisionCaptioner instance.

    Returns:
        VisionCaptioner: Singleton instance cá»§a VisionCaptioner
    """
    return VisionCaptioner()
