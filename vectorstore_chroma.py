"""
ChromaDB Vector Store Module
============================
Module quáº£n lÃ½ vector store vá»›i há»— trá»£ Hybrid Search (Dense + Sparse).

Kiáº¿n trÃºc:
- ChromaDB: LÆ°u trá»¯ dense vectors vÃ  metadata (persistent)
- In-memory Sparse Index: Inverted index cho sparse vectors (rebuild khi khá»Ÿi Ä‘á»™ng)

Hybrid Search:
- Dense Search: Cosine similarity trÃªn ChromaDB
- Sparse Search: BM25-like scoring trÃªn inverted index
- Combined Score: dense_weight * dense_score + sparse_weight * sparse_score

Táº¡i sao Hybrid?
- Dense: Tá»‘t cho semantic similarity (cÃ¢u cÃ³ nghÄ©a giá»‘ng nhau)
- Sparse: Tá»‘t cho exact keyword match (tÃªn thanh ghi, thuáº­t ngá»¯ ká»¹ thuáº­t)
- VÃ­ dá»¥: "GPIOA_ODR" sáº½ match tá»‘t hÆ¡n vá»›i sparse search

Singleton Pattern:
- Class VectorStore sá»­ dá»¥ng singleton
- ChromaDB client Ä‘Æ°á»£c khá»Ÿi táº¡o má»™t láº§n duy nháº¥t

Sá»­ dá»¥ng:
    from vectorstore_chroma import get_vectorstore

    vs = get_vectorstore()

    # ThÃªm documents
    chunks = [{"text": "...", "metadata": {...}}, ...]
    vs.add_documents(chunks, use_sparse=True)

    # TÃ¬m kiáº¿m
    results = vs.search("GPIO pin configuration", top_k=5, use_hybrid=True)
    # results = [{"id": ..., "text": ..., "score": ..., "metadata": ...}, ...]
"""

import chromadb
from chromadb.config import Settings
from typing import List, Dict, Any
import uuid
import json
from datetime import datetime

from config import (
    CHROMA_DIR, CHROMA_COLLECTION, TOP_K,
    DENSE_WEIGHT, SPARSE_WEIGHT, DEBUG_RETRIEVAL
)
from embedder import get_embedder


def log_retrieval_debug(message: str):
    """
    Log debug cho retrieval/search process.

    Chá»‰ hiá»ƒn thá»‹ khi DEBUG_RETRIEVAL=True trong config.
    Há»¯u Ã­ch Ä‘á»ƒ theo dÃµi:
    - Sá»‘ documents tÃ¬m Ä‘Æ°á»£c
    - Scores cá»§a tá»«ng káº¿t quáº£
    - Thá»i gian search

    Args:
        message: Ná»™i dung log debug
    """
    if DEBUG_RETRIEVAL:
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] ðŸ” RETRIEVAL: {message}")


class VectorStore:
    """
    Vector Store vá»›i Hybrid Search support.

    Singleton class quáº£n lÃ½ ChromaDB vÃ  sparse index.

    Attributes:
        client: ChromaDB PersistentClient
        collection: ChromaDB Collection cho dense vectors
        embedder: BGE-M3 Embedder instance
        sparse_index: Dict[str, List[tuple]] - Inverted index cho sparse search
                      {token: [(doc_id, weight), ...]}
        doc_sparse: Dict[str, Dict] - Sparse vectors cá»§a tá»«ng document
                    {doc_id: {token: weight, ...}}

    Storage:
        - Dense vectors + metadata: ChromaDB (persistent on disk)
        - Sparse index: In-memory (rebuild tá»« metadata khi khá»Ÿi Ä‘á»™ng)
    """
    _instance = None

    def __new__(cls):
        """
        Singleton pattern implementation.

        Returns:
            VectorStore: Instance duy nháº¥t cá»§a class
        """
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._init_store()
        return cls._instance

    def _init_store(self):
        """
        Khá»Ÿi táº¡o ChromaDB vÃ  sparse index.

        QuÃ¡ trÃ¬nh:
        1. Táº¡o ChromaDB PersistentClient (lÆ°u trÃªn disk)
        2. Láº¥y hoáº·c táº¡o collection vá»›i cosine distance
        3. Load embedder (BGE-M3)
        4. Rebuild sparse index tá»« metadata trong ChromaDB
        """
        print(f"Initializing ChromaDB at: {CHROMA_DIR}")

        self.client = chromadb.PersistentClient(
            path=str(CHROMA_DIR),
            settings=Settings(anonymized_telemetry=False)
        )
        self.embedder = get_embedder()

        # Collection cho dense vectors (cosine similarity)
        self.collection = self.client.get_or_create_collection(
            name=CHROMA_COLLECTION,
            metadata={"hnsw:space": "cosine"}  # HNSW index vá»›i cosine distance
        )

        # In-memory sparse index (simple inverted index)
        # token -> [(doc_id, weight), ...]
        self.sparse_index: Dict[str, List[tuple]] = {}
        # doc_id -> sparse_vector
        self.doc_sparse: Dict[str, Dict] = {}

        self._load_sparse_index()
        print(f"Collection '{CHROMA_COLLECTION}' ready, docs: {self.collection.count()}")
        log_retrieval_debug(
            f"Initialized: {self.collection.count()} docs, "
            f"{len(self.sparse_index)} unique tokens in sparse index"
        )

    def _load_sparse_index(self):
        """
        Load sparse index tá»« metadata trong ChromaDB.

        Sparse vectors Ä‘Æ°á»£c lÆ°u dÆ°á»›i dáº¡ng JSON string trong metadata
        cá»§a má»—i document. Khi khá»Ÿi Ä‘á»™ng, rebuild inverted index tá»«
        táº¥t cáº£ documents.

        QuÃ¡ trÃ¬nh:
        1. Láº¥y táº¥t cáº£ documents tá»« ChromaDB
        2. Parse sparse_vector tá»« metadata
        3. Build inverted index: token -> [(doc_id, weight), ...]
        """
        try:
            all_docs = self.collection.get(include=["metadatas"])

            for doc_id, metadata in zip(all_docs["ids"], all_docs["metadatas"]):
                if metadata and "sparse_vector" in metadata:
                    sparse = json.loads(metadata["sparse_vector"])
                    self.doc_sparse[doc_id] = sparse

                    for token, weight in sparse.items():
                        if token not in self.sparse_index:
                            self.sparse_index[token] = []
                        self.sparse_index[token].append((doc_id, weight))

            log_retrieval_debug(
                f"Loaded sparse index: {len(self.doc_sparse)} docs, "
                f"{len(self.sparse_index)} unique tokens"
            )
        except Exception as e:
            print(f"Warning: Could not load sparse index: {e}")

    def add_documents(self, chunks: List[Dict[str, Any]], use_sparse: bool = True) -> int:
        """
        ThÃªm documents vÃ o vector store.

        QuÃ¡ trÃ¬nh:
        1. Embed táº¥t cáº£ texts (dense + optional sparse)
        2. Serialize sparse vectors vÃ o metadata
        3. Update in-memory sparse index
        4. Add vÃ o ChromaDB

        Args:
            chunks: List cÃ¡c chunks, má»—i chunk lÃ  dict:
                    {"text": str, "metadata": dict}
            use_sparse: CÃ³ táº¡o sparse vectors khÃ´ng (default: True)

        Returns:
            int: Sá»‘ chunks Ä‘Ã£ thÃªm

        Example:
            >>> chunks = [
            ...     {"text": "GPIO configuration...", "metadata": {"source": "doc.pdf"}},
            ...     {"text": "Timer setup...", "metadata": {"source": "doc.pdf"}}
            ... ]
            >>> count = vs.add_documents(chunks, use_sparse=True)
            >>> print(f"Added {count} chunks")
        """
        if not chunks:
            return 0

        texts = [c["text"] for c in chunks]
        metadatas = [c.get("metadata", {}) for c in chunks]
        ids = [str(uuid.uuid4()) for _ in chunks]

        log_retrieval_debug(f"Adding {len(chunks)} documents, use_sparse={use_sparse}")

        # Embed vá»›i cáº£ dense vÃ  sparse
        embeddings_result = self.embedder.embed(texts, return_sparse=use_sparse)
        dense_embeddings = embeddings_result["dense"].tolist()

        # Xá»­ lÃ½ sparse vectors
        if use_sparse and embeddings_result.get("sparse"):
            sparse_vectors = embeddings_result["sparse"]

            for i, (doc_id, sparse) in enumerate(zip(ids, sparse_vectors)):
                # Chuyá»ƒn sparse vector thÃ nh serializable format
                sparse_dict = {str(k): float(v) for k, v in sparse.items()}
                metadatas[i]["sparse_vector"] = json.dumps(sparse_dict)

                # Update in-memory index
                self.doc_sparse[doc_id] = sparse_dict
                for token, weight in sparse_dict.items():
                    if token not in self.sparse_index:
                        self.sparse_index[token] = []
                    self.sparse_index[token].append((doc_id, weight))

        # Add vÃ o ChromaDB
        self.collection.add(
            ids=ids,
            embeddings=dense_embeddings,
            documents=texts,
            metadatas=metadatas
        )

        log_retrieval_debug(
            f"Added {len(chunks)} docs, total: {self.collection.count()}"
        )

        return len(chunks)

    def _sparse_search(self, query_sparse: Dict, top_k: int) -> Dict[str, float]:
        """
        TÃ¬m kiáº¿m báº±ng sparse vector (BM25-like).

        Scoring:
        - Vá»›i má»—i token trong query, tÃ¬m documents chá»©a token Ä‘Ã³
        - Score = sum(query_weight * doc_weight) cho má»—i token match

        Args:
            query_sparse: Sparse vector cá»§a query {token: weight}
            top_k: Sá»‘ káº¿t quáº£ tá»‘i Ä‘a (khÃ´ng dÃ¹ng á»Ÿ Ä‘Ã¢y, Ä‘á»ƒ filter sau)

        Returns:
            Dict[str, float]: {doc_id: score} cho táº¥t cáº£ docs cÃ³ match
        """
        scores = {}

        for token, query_weight in query_sparse.items():
            token_str = str(token)
            if token_str in self.sparse_index:
                for doc_id, doc_weight in self.sparse_index[token_str]:
                    if doc_id not in scores:
                        scores[doc_id] = 0.0
                    scores[doc_id] += query_weight * doc_weight

        log_retrieval_debug(
            f"Sparse search: {len(query_sparse)} query tokens, "
            f"matched {len(scores)} docs"
        )

        return scores

    def search(
        self,
        query: str,
        top_k: int = TOP_K,
        use_hybrid: bool = True,
        dense_weight: float = DENSE_WEIGHT,
        sparse_weight: float = SPARSE_WEIGHT
    ) -> List[Dict]:
        """
        Hybrid search: káº¿t há»£p Dense vÃ  Sparse.

        QuÃ¡ trÃ¬nh:
        1. Embed query (dense + sparse náº¿u hybrid)
        2. Dense search qua ChromaDB (cosine similarity)
        3. Sparse search qua inverted index (náº¿u hybrid)
        4. Normalize vÃ  combine scores
        5. Sort vÃ  tráº£ vá» top_k

        Scoring formula (hybrid):
            combined_score = dense_weight * normalized_dense + sparse_weight * normalized_sparse

        Args:
            query: CÃ¢u query tÃ¬m kiáº¿m
            top_k: Sá»‘ káº¿t quáº£ tráº£ vá» (default tá»« config)
            use_hybrid: CÃ³ dÃ¹ng hybrid search khÃ´ng (default: True)
            dense_weight: Trá»ng sá»‘ cho dense score (default: 0.7)
            sparse_weight: Trá»ng sá»‘ cho sparse score (default: 0.3)
                          dense_weight + sparse_weight nÃªn = 1.0

        Returns:
            List[Dict]: Danh sÃ¡ch káº¿t quáº£, má»—i item cÃ³:
            - id: Document ID
            - text: Ná»™i dung document
            - score: Combined score (hoáº·c dense score náº¿u khÃ´ng hybrid)
            - metadata: Metadata cá»§a document
            - dense_score: (chá»‰ cÃ³ náº¿u hybrid) Dense similarity score
            - sparse_score: (chá»‰ cÃ³ náº¿u hybrid) Sparse match score

        Example:
            >>> results = vs.search("GPIO input mode", top_k=5, use_hybrid=True)
            >>> for r in results:
            ...     print(f"{r['score']:.3f} - {r['metadata']['source']}")
        """
        log_retrieval_debug(
            f"Search: '{query[:50]}...', top_k={top_k}, "
            f"hybrid={use_hybrid}, weights=({dense_weight}/{sparse_weight})"
        )

        # Embed query
        query_result = self.embedder.embed_query(query, return_sparse=use_hybrid)
        query_dense = query_result["dense"].tolist()

        # Dense search qua ChromaDB
        # Láº¥y nhiá»u hÆ¡n náº¿u hybrid Ä‘á»ƒ cÃ³ thá»ƒ merge vá»›i sparse results
        n_results = min(top_k * 2, self.collection.count()) if use_hybrid else top_k
        if n_results == 0:
            log_retrieval_debug("Collection empty, returning empty results")
            return []

        dense_results = self.collection.query(
            query_embeddings=[query_dense],
            n_results=n_results,
            include=["documents", "metadatas", "distances"]
        )

        # Parse dense results
        dense_scores = {}
        doc_data = {}

        for i in range(len(dense_results["ids"][0])):
            doc_id = dense_results["ids"][0][i]
            # ChromaDB tráº£ vá» distance, convert thÃ nh similarity
            # cosine distance = 1 - cosine similarity
            dense_scores[doc_id] = 1 - dense_results["distances"][0][i]
            doc_data[doc_id] = {
                "id": doc_id,
                "text": dense_results["documents"][0][i],
                "metadata": dense_results["metadatas"][0][i]
            }

        log_retrieval_debug(f"Dense search: found {len(dense_scores)} docs")

        if not use_hybrid or not query_result.get("sparse"):
            # Chá»‰ dÃ¹ng dense - tráº£ vá» káº¿t quáº£ trá»±c tiáº¿p
            docs = []
            for doc_id in dense_results["ids"][0][:top_k]:
                doc = doc_data[doc_id]
                doc["score"] = dense_scores[doc_id]
                # Remove sparse_vector tá»« metadata Ä‘á»ƒ response gá»n hÆ¡n
                if "sparse_vector" in doc["metadata"]:
                    del doc["metadata"]["sparse_vector"]
                docs.append(doc)

            log_retrieval_debug(
                f"Dense-only results: {len(docs)} docs, "
                f"top score={docs[0]['score']:.3f}" if docs else "no results"
            )

            return docs

        # Sparse search
        sparse_scores = self._sparse_search(query_result["sparse"], top_k * 2)

        # Normalize scores (Ä‘á»ƒ káº¿t há»£p Ä‘Æ°á»£c)
        max_dense = max(dense_scores.values()) if dense_scores else 1.0
        max_sparse = max(sparse_scores.values()) if sparse_scores else 1.0

        # Combine scores
        all_doc_ids = set(dense_scores.keys()) | set(sparse_scores.keys())
        combined_scores = {}

        for doc_id in all_doc_ids:
            d_score = dense_scores.get(doc_id, 0.0) / max_dense
            s_score = sparse_scores.get(doc_id, 0.0) / max_sparse
            combined_scores[doc_id] = dense_weight * d_score + sparse_weight * s_score

        # Sort vÃ  láº¥y top_k
        sorted_docs = sorted(
            combined_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]

        # Fetch document data náº¿u chÆ°a cÃ³ (tá»« sparse-only matches)
        missing_ids = [doc_id for doc_id, _ in sorted_docs if doc_id not in doc_data]
        if missing_ids:
            missing_data = self.collection.get(
                ids=missing_ids,
                include=["documents", "metadatas"]
            )
            for i, doc_id in enumerate(missing_data["ids"]):
                doc_data[doc_id] = {
                    "id": doc_id,
                    "text": missing_data["documents"][i],
                    "metadata": missing_data["metadatas"][i]
                }

        # Build result vá»›i full scoring info
        docs = []
        for doc_id, score in sorted_docs:
            if doc_id in doc_data:
                doc = doc_data[doc_id].copy()
                doc["score"] = score
                doc["dense_score"] = dense_scores.get(doc_id, 0.0)
                doc["sparse_score"] = sparse_scores.get(doc_id, 0.0)
                # Remove sparse_vector tá»« metadata
                if "sparse_vector" in doc["metadata"]:
                    del doc["metadata"]["sparse_vector"]
                docs.append(doc)

        # Log top-k results khi DEBUG_RETRIEVAL
        if DEBUG_RETRIEVAL and docs:
            log_retrieval_debug(f"Hybrid results ({len(docs)} docs):")
            for i, d in enumerate(docs[:5]):  # Log top 5
                source = d["metadata"].get("source", "?")
                log_retrieval_debug(
                    f"  [{i+1}] score={d['score']:.3f} "
                    f"(dense={d['dense_score']:.3f}, sparse={d['sparse_score']:.3f}) "
                    f"- {source}"
                )

        return docs

    def delete_by_source(self, source: str) -> int:
        """
        XÃ³a documents theo source file.

        QuÃ¡ trÃ¬nh:
        1. TÃ¬m táº¥t cáº£ documents cÃ³ metadata.source = source
        2. Remove tá»« sparse index (in-memory)
        3. Remove tá»« ChromaDB

        Args:
            source: TÃªn file nguá»“n (vd: "document.pdf")

        Returns:
            int: Sá»‘ chunks Ä‘Ã£ xÃ³a
        """
        try:
            results = self.collection.get(
                where={"source": source},
                include=["metadatas"]
            )

            if results["ids"]:
                log_retrieval_debug(f"Deleting {len(results['ids'])} docs with source={source}")

                # Remove tá»« sparse index
                for doc_id in results["ids"]:
                    if doc_id in self.doc_sparse:
                        sparse = self.doc_sparse[doc_id]
                        for token in sparse.keys():
                            if token in self.sparse_index:
                                self.sparse_index[token] = [
                                    (d, w) for d, w in self.sparse_index[token]
                                    if d != doc_id
                                ]
                        del self.doc_sparse[doc_id]

                # Remove tá»« ChromaDB
                self.collection.delete(ids=results["ids"])
                print(f"Deleted {len(results['ids'])} chunks with source={source}")
                return len(results["ids"])

            return 0
        except Exception as e:
            print(f"Delete error: {e}")
            return 0

    def get_stats(self) -> Dict:
        """
        Láº¥y thá»‘ng kÃª cá»§a collection.

        Returns:
            Dict vá»›i cÃ¡c thÃ´ng tin:
            - total_documents: Tá»•ng sá»‘ documents
            - collection_name: TÃªn collection
            - sparse_index_tokens: Sá»‘ unique tokens trong sparse index
            - hybrid_enabled: LuÃ´n True (há»‡ thá»‘ng há»— trá»£ hybrid)
        """
        return {
            "total_documents": self.collection.count(),
            "collection_name": CHROMA_COLLECTION,
            "sparse_index_tokens": len(self.sparse_index),
            "hybrid_enabled": True
        }


def get_vectorstore() -> VectorStore:
    """
    Factory function Ä‘á»ƒ láº¥y VectorStore instance.

    Sá»­ dá»¥ng function nÃ y thay vÃ¬ gá»i VectorStore() trá»±c tiáº¿p
    Ä‘á»ƒ Ä‘áº£m báº£o singleton pattern.

    Returns:
        VectorStore: Singleton instance cá»§a VectorStore

    Example:
        >>> vs = get_vectorstore()
        >>> vs2 = get_vectorstore()
        >>> vs is vs2
        True
    """
    return VectorStore()
