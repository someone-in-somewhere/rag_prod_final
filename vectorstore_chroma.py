"""
ChromaDB Vector Store Module
============================
Module qu·∫£n l√Ω vector store v·ªõi h·ªó tr·ª£ Hybrid Search (Dense + Sparse).

Ki·∫øn tr√∫c:
- ChromaDB: L∆∞u tr·ªØ dense vectors v√† metadata (persistent)
- In-memory Sparse Index: Inverted index cho sparse vectors (rebuild khi kh·ªüi ƒë·ªông)

Hybrid Search:
- Dense Search: Cosine similarity tr√™n ChromaDB
- Sparse Search: BM25-like scoring tr√™n inverted index
- Combined Score: dense_weight * dense_score + sparse_weight * sparse_score

T·∫°i sao Hybrid?
- Dense: T·ªët cho semantic similarity (c√¢u c√≥ nghƒ©a gi·ªëng nhau)
- Sparse: T·ªët cho exact keyword match (t√™n thanh ghi, thu·∫≠t ng·ªØ k·ªπ thu·∫≠t)
- V√≠ d·ª•: "GPIOA_ODR" s·∫Ω match t·ªët h∆°n v·ªõi sparse search

Singleton Pattern:
- Class VectorStore s·ª≠ d·ª•ng singleton
- ChromaDB client ƒë∆∞·ª£c kh·ªüi t·∫°o m·ªôt l·∫ßn duy nh·∫•t

S·ª≠ d·ª•ng:
    from vectorstore_chroma import get_vectorstore

    vs = get_vectorstore()

    # Th√™m documents
    chunks = [{"text": "...", "metadata": {...}}, ...]
    vs.add_documents(chunks, use_sparse=True)

    # T√¨m ki·∫øm
    results = vs.search("GPIO pin configuration", top_k=5, use_hybrid=True)
    # results = [{"id": ..., "text": ..., "score": ..., "metadata": ...}, ...]
"""

import chromadb
from chromadb.config import Settings
from typing import List, Dict, Any
import uuid
import json
from datetime import datetime

from config import (
    CHROMA_DIR, CHROMA_COLLECTION, TOP_K,
    DENSE_WEIGHT, SPARSE_WEIGHT, DEBUG_RETRIEVAL
)
from embedder import get_embedder


def log_retrieval_debug(message: str):
    """
    Log debug cho retrieval/search process.

    Ch·ªâ hi·ªÉn th·ªã khi DEBUG_RETRIEVAL=True trong config.
    H·ªØu √≠ch ƒë·ªÉ theo d√µi:
    - S·ªë documents t√¨m ƒë∆∞·ª£c
    - Scores c·ªßa t·ª´ng k·∫øt qu·∫£
    - Th·ªùi gian search

    Args:
        message: N·ªôi dung log debug
    """
    if DEBUG_RETRIEVAL:
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] üîç RETRIEVAL: {message}")


class VectorStore:
    """
    Vector Store v·ªõi Hybrid Search support.

    Singleton class qu·∫£n l√Ω ChromaDB v√† sparse index.

    Attributes:
        client: ChromaDB PersistentClient
        collection: ChromaDB Collection cho dense vectors
        embedder: BGE-M3 Embedder instance
        sparse_index: Dict[str, List[tuple]] - Inverted index cho sparse search
                      {token: [(doc_id, weight), ...]}
        doc_sparse: Dict[str, Dict] - Sparse vectors c·ªßa t·ª´ng document
                    {doc_id: {token: weight, ...}}

    Storage:
        - Dense vectors + metadata: ChromaDB (persistent on disk)
        - Sparse index: In-memory (rebuild t·ª´ metadata khi kh·ªüi ƒë·ªông)
    """
    _instance = None

    def __new__(cls):
        """
        Singleton pattern implementation.

        Returns:
            VectorStore: Instance duy nh·∫•t c·ªßa class
        """
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._init_store()
        return cls._instance

    def _init_store(self):
        """
        Kh·ªüi t·∫°o ChromaDB v√† sparse index.

        Qu√° tr√¨nh:
        1. T·∫°o ChromaDB PersistentClient (l∆∞u tr√™n disk)
        2. L·∫•y ho·∫∑c t·∫°o collection v·ªõi cosine distance
        3. Load embedder (BGE-M3)
        4. Rebuild sparse index t·ª´ metadata trong ChromaDB
        """
        print(f"Initializing ChromaDB at: {CHROMA_DIR}")

        self.client = chromadb.PersistentClient(
            path=str(CHROMA_DIR),
            settings=Settings(anonymized_telemetry=False)
        )
        self.embedder = get_embedder()

        # Collection cho dense vectors (cosine similarity)
        self.collection = self.client.get_or_create_collection(
            name=CHROMA_COLLECTION,
            metadata={"hnsw:space": "cosine"}  # HNSW index v·ªõi cosine distance
        )

        # In-memory sparse index (simple inverted index)
        # token -> [(doc_id, weight), ...]
        self.sparse_index: Dict[str, List[tuple]] = {}
        # doc_id -> sparse_vector
        self.doc_sparse: Dict[str, Dict] = {}

        self._load_sparse_index()
        print(f"Collection '{CHROMA_COLLECTION}' ready, docs: {self.collection.count()}")
        log_retrieval_debug(
            f"Initialized: {self.collection.count()} docs, "
            f"{len(self.sparse_index)} unique tokens in sparse index"
        )

    def _load_sparse_index(self):
        """
        Load sparse index t·ª´ metadata trong ChromaDB.

        Sparse vectors ƒë∆∞·ª£c l∆∞u d∆∞·ªõi d·∫°ng JSON string trong metadata
        c·ªßa m·ªói document. Khi kh·ªüi ƒë·ªông, rebuild inverted index t·ª´
        t·∫•t c·∫£ documents.

        Qu√° tr√¨nh:
        1. L·∫•y t·∫•t c·∫£ documents t·ª´ ChromaDB
        2. Parse sparse_vector t·ª´ metadata
        3. Build inverted index: token -> [(doc_id, weight), ...]
        """
        try:
            all_docs = self.collection.get(include=["metadatas"])

            for doc_id, metadata in zip(all_docs["ids"], all_docs["metadatas"]):
                if metadata and "sparse_vector" in metadata:
                    sparse = json.loads(metadata["sparse_vector"])
                    self.doc_sparse[doc_id] = sparse

                    for token, weight in sparse.items():
                        if token not in self.sparse_index:
                            self.sparse_index[token] = []
                        self.sparse_index[token].append((doc_id, weight))

            log_retrieval_debug(
                f"Loaded sparse index: {len(self.doc_sparse)} docs, "
                f"{len(self.sparse_index)} unique tokens"
            )
        except Exception as e:
            print(f"Warning: Could not load sparse index: {e}")

    def add_documents(self, chunks: List[Dict[str, Any]], use_sparse: bool = True) -> int:
        """
        Th√™m documents v√†o vector store.

        Qu√° tr√¨nh:
        1. Embed t·∫•t c·∫£ texts (dense + optional sparse)
        2. Serialize sparse vectors v√†o metadata
        3. Update in-memory sparse index
        4. Add v√†o ChromaDB

        Args:
            chunks: List c√°c chunks, m·ªói chunk l√† dict:
                    {"text": str, "metadata": dict}
            use_sparse: C√≥ t·∫°o sparse vectors kh√¥ng (default: True)

        Returns:
            int: S·ªë chunks ƒë√£ th√™m

        Example:
            >>> chunks = [
            ...     {"text": "GPIO configuration...", "metadata": {"source": "doc.pdf"}},
            ...     {"text": "Timer setup...", "metadata": {"source": "doc.pdf"}}
            ... ]
            >>> count = vs.add_documents(chunks, use_sparse=True)
            >>> print(f"Added {count} chunks")
        """
        if not chunks:
            return 0

        texts = [c["text"] for c in chunks]
        metadatas = [c.get("metadata", {}) for c in chunks]
        ids = [str(uuid.uuid4()) for _ in chunks]

        log_retrieval_debug(f"Adding {len(chunks)} documents, use_sparse={use_sparse}")

        # Embed v·ªõi c·∫£ dense v√† sparse
        embeddings_result = self.embedder.embed(texts, return_sparse=use_sparse)
        dense_embeddings = embeddings_result["dense"].tolist()

        # X·ª≠ l√Ω sparse vectors
        if use_sparse and embeddings_result.get("sparse"):
            sparse_vectors = embeddings_result["sparse"]

            for i, (doc_id, sparse) in enumerate(zip(ids, sparse_vectors)):
                # Chuy·ªÉn sparse vector th√†nh serializable format
                sparse_dict = {str(k): float(v) for k, v in sparse.items()}
                metadatas[i]["sparse_vector"] = json.dumps(sparse_dict)

                # Update in-memory index
                self.doc_sparse[doc_id] = sparse_dict
                for token, weight in sparse_dict.items():
                    if token not in self.sparse_index:
                        self.sparse_index[token] = []
                    self.sparse_index[token].append((doc_id, weight))

        # Add v√†o ChromaDB
        self.collection.add(
            ids=ids,
            embeddings=dense_embeddings,
            documents=texts,
            metadatas=metadatas
        )

        log_retrieval_debug(
            f"Added {len(chunks)} docs, total: {self.collection.count()}"
        )

        return len(chunks)

    def _sparse_search(self, query_sparse: Dict, top_k: int) -> Dict[str, float]:
        """
        T√¨m ki·∫øm b·∫±ng sparse vector (BM25-like).

        Scoring:
        - V·ªõi m·ªói token trong query, t√¨m documents ch·ª©a token ƒë√≥
        - Score = sum(query_weight * doc_weight) cho m·ªói token match

        Args:
            query_sparse: Sparse vector c·ªßa query {token: weight}
            top_k: S·ªë k·∫øt qu·∫£ t·ªëi ƒëa (kh√¥ng d√πng ·ªü ƒë√¢y, ƒë·ªÉ filter sau)

        Returns:
            Dict[str, float]: {doc_id: score} cho t·∫•t c·∫£ docs c√≥ match
        """
        scores = {}

        for token, query_weight in query_sparse.items():
            token_str = str(token)
            if token_str in self.sparse_index:
                for doc_id, doc_weight in self.sparse_index[token_str]:
                    if doc_id not in scores:
                        scores[doc_id] = 0.0
                    scores[doc_id] += query_weight * doc_weight

        log_retrieval_debug(
            f"Sparse search: {len(query_sparse)} query tokens, "
            f"matched {len(scores)} docs"
        )

        return scores

    def search(
        self,
        query: str,
        top_k: int = TOP_K,
        use_hybrid: bool = True,
        dense_weight: float = DENSE_WEIGHT,
        sparse_weight: float = SPARSE_WEIGHT
    ) -> List[Dict]:
        """
        Hybrid search: k·∫øt h·ª£p Dense v√† Sparse.

        Qu√° tr√¨nh:
        1. Embed query (dense + sparse n·∫øu hybrid)
        2. Dense search qua ChromaDB (cosine similarity)
        3. Sparse search qua inverted index (n·∫øu hybrid)
        4. Normalize v√† combine scores
        5. Sort v√† tr·∫£ v·ªÅ top_k

        Scoring formula (hybrid):
            combined_score = dense_weight * normalized_dense + sparse_weight * normalized_sparse

        Args:
            query: C√¢u query t√¨m ki·∫øm
            top_k: S·ªë k·∫øt qu·∫£ tr·∫£ v·ªÅ (default t·ª´ config)
            use_hybrid: C√≥ d√πng hybrid search kh√¥ng (default: True)
            dense_weight: Tr·ªçng s·ªë cho dense score (default: 0.7)
            sparse_weight: Tr·ªçng s·ªë cho sparse score (default: 0.3)
                          dense_weight + sparse_weight n√™n = 1.0

        Returns:
            List[Dict]: Danh s√°ch k·∫øt qu·∫£, m·ªói item c√≥:
            - id: Document ID
            - text: N·ªôi dung document
            - score: Combined score (ho·∫∑c dense score n·∫øu kh√¥ng hybrid)
            - metadata: Metadata c·ªßa document
            - dense_score: (ch·ªâ c√≥ n·∫øu hybrid) Dense similarity score
            - sparse_score: (ch·ªâ c√≥ n·∫øu hybrid) Sparse match score

        Example:
            >>> results = vs.search("GPIO input mode", top_k=5, use_hybrid=True)
            >>> for r in results:
            ...     print(f"{r['score']:.3f} - {r['metadata']['source']}")
        """
        log_retrieval_debug(
            f"Search: '{query[:50]}...', top_k={top_k}, "
            f"hybrid={use_hybrid}, weights=({dense_weight}/{sparse_weight})"
        )

        # Embed query
        query_result = self.embedder.embed_query(query, return_sparse=use_hybrid)
        query_dense = query_result["dense"].tolist()

        # Dense search qua ChromaDB
        # L·∫•y nhi·ªÅu h∆°n n·∫øu hybrid ƒë·ªÉ c√≥ th·ªÉ merge v·ªõi sparse results
        # QUAN TR·ªåNG: n_results ph·∫£i <= s·ªë docs trong collection, n·∫øu kh√¥ng HNSW s·∫Ω l·ªói
        collection_count = self.collection.count()
        if collection_count == 0:
            log_retrieval_debug("Collection empty, returning empty results")
            return []

        n_results = min(top_k * 2 if use_hybrid else top_k, collection_count)

        dense_results = self.collection.query(
            query_embeddings=[query_dense],
            n_results=n_results,
            include=["documents", "metadatas", "distances"]
        )

        # Parse dense results
        dense_scores = {}
        doc_data = {}

        for i in range(len(dense_results["ids"][0])):
            doc_id = dense_results["ids"][0][i]
            # ChromaDB tr·∫£ v·ªÅ distance, convert th√†nh similarity
            # cosine distance = 1 - cosine similarity
            dense_scores[doc_id] = 1 - dense_results["distances"][0][i]
            doc_data[doc_id] = {
                "id": doc_id,
                "text": dense_results["documents"][0][i],
                "metadata": dense_results["metadatas"][0][i]
            }

        log_retrieval_debug(f"Dense search: found {len(dense_scores)} docs")

        if not use_hybrid or not query_result.get("sparse"):
            # Ch·ªâ d√πng dense - tr·∫£ v·ªÅ k·∫øt qu·∫£ tr·ª±c ti·∫øp
            docs = []
            for doc_id in dense_results["ids"][0][:top_k]:
                doc = doc_data[doc_id]
                doc["score"] = dense_scores[doc_id]
                # Remove sparse_vector t·ª´ metadata ƒë·ªÉ response g·ªçn h∆°n
                if "sparse_vector" in doc["metadata"]:
                    del doc["metadata"]["sparse_vector"]
                docs.append(doc)

            log_retrieval_debug(
                f"Dense-only results: {len(docs)} docs, "
                f"top score={docs[0]['score']:.3f}" if docs else "no results"
            )

            return docs

        # Sparse search
        sparse_scores = self._sparse_search(query_result["sparse"], top_k * 2)

        # Normalize scores (ƒë·ªÉ k·∫øt h·ª£p ƒë∆∞·ª£c)
        max_dense = max(dense_scores.values()) if dense_scores else 1.0
        max_sparse = max(sparse_scores.values()) if sparse_scores else 1.0

        # Combine scores
        all_doc_ids = set(dense_scores.keys()) | set(sparse_scores.keys())
        combined_scores = {}

        for doc_id in all_doc_ids:
            d_score = dense_scores.get(doc_id, 0.0) / max_dense
            s_score = sparse_scores.get(doc_id, 0.0) / max_sparse
            combined_scores[doc_id] = dense_weight * d_score + sparse_weight * s_score

        # Sort v√† l·∫•y top_k
        sorted_docs = sorted(
            combined_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:top_k]

        # Fetch document data n·∫øu ch∆∞a c√≥ (t·ª´ sparse-only matches)
        missing_ids = [doc_id for doc_id, _ in sorted_docs if doc_id not in doc_data]
        if missing_ids:
            missing_data = self.collection.get(
                ids=missing_ids,
                include=["documents", "metadatas"]
            )
            for i, doc_id in enumerate(missing_data["ids"]):
                doc_data[doc_id] = {
                    "id": doc_id,
                    "text": missing_data["documents"][i],
                    "metadata": missing_data["metadatas"][i]
                }

        # Build result v·ªõi full scoring info
        docs = []
        for doc_id, score in sorted_docs:
            if doc_id in doc_data:
                doc = doc_data[doc_id].copy()
                doc["score"] = score
                doc["dense_score"] = dense_scores.get(doc_id, 0.0)
                doc["sparse_score"] = sparse_scores.get(doc_id, 0.0)
                # Remove sparse_vector t·ª´ metadata
                if "sparse_vector" in doc["metadata"]:
                    del doc["metadata"]["sparse_vector"]
                docs.append(doc)

        # Log top-k results khi DEBUG_RETRIEVAL
        if DEBUG_RETRIEVAL and docs:
            log_retrieval_debug(f"Hybrid results ({len(docs)} docs):")
            for i, d in enumerate(docs[:5]):  # Log top 5
                source = d["metadata"].get("source", "?")
                log_retrieval_debug(
                    f"  [{i+1}] score={d['score']:.3f} "
                    f"(dense={d['dense_score']:.3f}, sparse={d['sparse_score']:.3f}) "
                    f"- {source}"
                )

        return docs

    def delete_by_source(self, source: str) -> int:
        """
        X√≥a documents theo source file.

        Qu√° tr√¨nh:
        1. T√¨m t·∫•t c·∫£ documents c√≥ metadata.source = source
        2. Remove t·ª´ sparse index (in-memory)
        3. Remove t·ª´ ChromaDB

        Args:
            source: T√™n file ngu·ªìn (vd: "document.pdf")

        Returns:
            int: S·ªë chunks ƒë√£ x√≥a
        """
        try:
            results = self.collection.get(
                where={"source": source},
                include=["metadatas"]
            )

            if results["ids"]:
                log_retrieval_debug(f"Deleting {len(results['ids'])} docs with source={source}")

                # Remove t·ª´ sparse index
                for doc_id in results["ids"]:
                    if doc_id in self.doc_sparse:
                        sparse = self.doc_sparse[doc_id]
                        for token in sparse.keys():
                            if token in self.sparse_index:
                                self.sparse_index[token] = [
                                    (d, w) for d, w in self.sparse_index[token]
                                    if d != doc_id
                                ]
                        del self.doc_sparse[doc_id]

                # Remove t·ª´ ChromaDB
                self.collection.delete(ids=results["ids"])
                print(f"Deleted {len(results['ids'])} chunks with source={source}")
                return len(results["ids"])

            return 0
        except Exception as e:
            print(f"Delete error: {e}")
            return 0

    def get_stats(self) -> Dict:
        """
        L·∫•y th·ªëng k√™ c·ªßa collection.

        Returns:
            Dict v·ªõi c√°c th√¥ng tin:
            - total_documents: T·ªïng s·ªë documents
            - collection_name: T√™n collection
            - sparse_index_tokens: S·ªë unique tokens trong sparse index
            - hybrid_enabled: Lu√¥n True (h·ªá th·ªëng h·ªó tr·ª£ hybrid)
        """
        return {
            "total_documents": self.collection.count(),
            "collection_name": CHROMA_COLLECTION,
            "sparse_index_tokens": len(self.sparse_index),
            "hybrid_enabled": True
        }


def get_vectorstore() -> VectorStore:
    """
    Factory function ƒë·ªÉ l·∫•y VectorStore instance.

    S·ª≠ d·ª•ng function n√†y thay v√¨ g·ªçi VectorStore() tr·ª±c ti·∫øp
    ƒë·ªÉ ƒë·∫£m b·∫£o singleton pattern.

    Returns:
        VectorStore: Singleton instance c·ªßa VectorStore

    Example:
        >>> vs = get_vectorstore()
        >>> vs2 = get_vectorstore()
        >>> vs is vs2
        True
    """
    return VectorStore()
