"""
BGE-M3 Embedding Module
=======================
Module xá»­ lÃ½ embedding vÄƒn báº£n sá»­ dá»¥ng model BGE-M3.

BGE-M3 lÃ  model embedding Ä‘a ngÃ´n ngá»¯ há»— trá»£:
- Dense embedding: Vector 1024 chiá»u, dÃ¹ng cho semantic similarity
- Sparse embedding (Lexical weights): TÆ°Æ¡ng tá»± BM25, dÃ¹ng cho keyword matching
- ColBERT vectors: Multi-vector representation (khÃ´ng dÃ¹ng trong há»‡ thá»‘ng nÃ y)

Hybrid Search:
- Káº¿t há»£p dense + sparse Ä‘á»ƒ cáº£i thiá»‡n retrieval
- Dense: Tá»‘t cho semantic (nghÄ©a tÆ°Æ¡ng Ä‘á»“ng)
- Sparse: Tá»‘t cho keyword exact match (thuáº­t ngá»¯ ká»¹ thuáº­t, tÃªn thanh ghi)

Singleton Pattern:
- Class Embedder sá»­ dá»¥ng singleton Ä‘á»ƒ trÃ¡nh load model nhiá»u láº§n
- Model Ä‘Æ°á»£c cache trong _instance

Sá»­ dá»¥ng:
    from embedder import get_embedder
    embedder = get_embedder()

    # Embed nhiá»u vÄƒn báº£n
    result = embedder.embed(["text1", "text2"], return_sparse=True)
    # result["dense"] = np.ndarray shape (2, 1024)
    # result["sparse"] = [{"token_id": weight}, ...]

    # Embed má»™t query
    query_vec = embedder.embed_query("tÃ¬m kiáº¿m gÃ¬ Ä‘Ã³", return_sparse=True)
    # query_vec["dense"] = np.ndarray shape (1024,)
    # query_vec["sparse"] = {"token_id": weight}
"""

from FlagEmbedding import BGEM3FlagModel
from typing import List, Dict
import numpy as np
from datetime import datetime

from config import EMBEDDING_MODEL, DEBUG_EMBEDDING


def log_embedding_debug(message: str):
    """
    Log debug cho embedding process.

    Chá»‰ hiá»ƒn thá»‹ khi DEBUG_EMBEDDING=True trong config.
    Há»¯u Ã­ch Ä‘á»ƒ theo dÃµi:
    - Sá»‘ lÆ°á»£ng text Ä‘Æ°á»£c embed
    - KÃ­ch thÆ°á»›c vector output
    - Thá»i gian embedding

    Args:
        message: Ná»™i dung log debug
    """
    if DEBUG_EMBEDDING:
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] ğŸ”¢ EMBED: {message}")


class Embedder:
    """
    BGE-M3 Embedder vá»›i há»— trá»£ Dense vÃ  Sparse vectors.

    Singleton class - chá»‰ cÃ³ má»™t instance duy nháº¥t Ä‘Æ°á»£c táº¡o.

    Attributes:
        model: BGEM3FlagModel instance
        dense_dim: KÃ­ch thÆ°á»›c dense vector (1024 cho BGE-M3)

    Model Configuration:
        - use_fp16=True: Sá»­ dá»¥ng FP16 Ä‘á»ƒ giáº£m VRAM (~2GB thay vÃ¬ 4GB)
        - device="cuda": Cháº¡y trÃªn GPU
    """
    _instance = None

    def __new__(cls):
        """
        Singleton pattern implementation.

        Returns:
            Embedder: Instance duy nháº¥t cá»§a class
        """
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._init_model()
        return cls._instance

    def _init_model(self):
        """
        Khá»Ÿi táº¡o BGE-M3 model.

        QuÃ¡ trÃ¬nh:
        1. Load model tá»« HuggingFace (hoáº·c cache local)
        2. Chuyá»ƒn sang FP16 Ä‘á»ƒ tiáº¿t kiá»‡m VRAM
        3. Load lÃªn GPU

        VRAM Usage: ~2GB vá»›i FP16
        """
        log_embedding_debug(f"Loading BGE-M3 model: {EMBEDDING_MODEL}")
        log_embedding_debug(f"Initializing model with FP16 on CUDA...")

        self.model = BGEM3FlagModel(
            EMBEDDING_MODEL,
            use_fp16=True,
            device="cuda"
        )
        self.dense_dim = 1024  # BGE-M3 dense dimension

        log_embedding_debug(f"BGE-M3 ready - Dense dim: {self.dense_dim}")

    def embed(self, texts: List[str], return_sparse: bool = False) -> Dict:
        """
        Embed má»™t danh sÃ¡ch vÄƒn báº£n.

        QuÃ¡ trÃ¬nh:
        1. Nháº­n list strings lÃ m input
        2. Gá»i model.encode() Ä‘á»ƒ táº¡o embeddings
        3. Tráº£ vá» dense vectors (luÃ´n cÃ³) vÃ  sparse vectors (náº¿u yÃªu cáº§u)

        Args:
            texts: Danh sÃ¡ch vÄƒn báº£n cáº§n embed
            return_sparse: CÃ³ tráº£ vá» sparse vectors khÃ´ng (default: False)

        Returns:
            Dict vá»›i cÃ¡c keys:
            - "dense": np.ndarray shape (N, 1024) - Dense vectors
            - "sparse": List[Dict] - Sparse vectors (náº¿u return_sparse=True)
                        Má»—i dict cÃ³ dáº¡ng {token_id: weight}

        Example:
            >>> embedder = get_embedder()
            >>> result = embedder.embed(["Hello world", "Xin chÃ o"], return_sparse=True)
            >>> result["dense"].shape
            (2, 1024)
            >>> len(result["sparse"])
            2

        Performance:
            - Batch size tá»± Ä‘á»™ng Ä‘Æ°á»£c model xá»­ lÃ½
            - GPU memory tá»· lá»‡ vá»›i sá»‘ texts
        """
        if not texts:
            log_embedding_debug("Empty input, returning empty arrays")
            return {"dense": np.array([]), "sparse": []}

        log_embedding_debug(f"Embedding {len(texts)} texts, return_sparse={return_sparse}")

        output = self.model.encode(
            texts,
            return_dense=True,
            return_sparse=return_sparse,
            return_colbert_vecs=False  # KhÃ´ng dÃ¹ng ColBERT vectors
        )

        result = {"dense": np.array(output["dense_vecs"])}

        if return_sparse and "lexical_weights" in output:
            result["sparse"] = output["lexical_weights"]
            log_embedding_debug(
                f"Output: dense shape={result['dense'].shape}, "
                f"sparse vectors={len(result['sparse'])}"
            )
        else:
            log_embedding_debug(f"Output: dense shape={result['dense'].shape}")

        return result

    def embed_query(self, query: str, return_sparse: bool = False) -> Dict:
        """
        Embed má»™t query (cÃ¢u há»i/tÃ¬m kiáº¿m).

        Wrapper cá»§a embed() cho single query, tráº£ vá» vector thay vÃ¬ list.

        Args:
            query: CÃ¢u query cáº§n embed
            return_sparse: CÃ³ tráº£ vá» sparse vector khÃ´ng

        Returns:
            Dict vá»›i cÃ¡c keys:
            - "dense": np.ndarray shape (1024,) - Dense vector
            - "sparse": Dict {token_id: weight} - Sparse vector (náº¿u return_sparse=True)

        Example:
            >>> vec = embedder.embed_query("GPIO lÃ  gÃ¬?", return_sparse=True)
            >>> vec["dense"].shape
            (1024,)
            >>> isinstance(vec["sparse"], dict)
            True
        """
        log_embedding_debug(f"Embedding query: '{query[:50]}...'")

        result = self.embed([query], return_sparse=return_sparse)
        return {
            "dense": result["dense"][0],
            "sparse": result["sparse"][0] if result.get("sparse") else {}
        }

    def embed_dense(self, texts: List[str]) -> np.ndarray:
        """
        Embed texts vÃ  chá»‰ tráº£ vá» dense vectors.

        Backward compatible function cho code cÅ© khÃ´ng dÃ¹ng hybrid search.

        Args:
            texts: Danh sÃ¡ch vÄƒn báº£n cáº§n embed

        Returns:
            np.ndarray: Dense vectors shape (N, 1024)
        """
        log_embedding_debug(f"embed_dense: {len(texts)} texts (dense only)")
        return self.embed(texts, return_sparse=False)["dense"]

    def embed_query_dense(self, query: str) -> np.ndarray:
        """
        Embed query vÃ  chá»‰ tráº£ vá» dense vector.

        Backward compatible function cho code cÅ© khÃ´ng dÃ¹ng hybrid search.

        Args:
            query: CÃ¢u query cáº§n embed

        Returns:
            np.ndarray: Dense vector shape (1024,)
        """
        log_embedding_debug(f"embed_query_dense: '{query[:50]}...' (dense only)")
        return self.embed_query(query, return_sparse=False)["dense"]


def get_embedder() -> Embedder:
    """
    Factory function Ä‘á»ƒ láº¥y Embedder instance.

    Sá»­ dá»¥ng function nÃ y thay vÃ¬ gá»i Embedder() trá»±c tiáº¿p
    Ä‘á»ƒ Ä‘áº£m báº£o singleton pattern.

    Returns:
        Embedder: Singleton instance cá»§a Embedder

    Example:
        >>> embedder = get_embedder()
        >>> embedder2 = get_embedder()
        >>> embedder is embedder2
        True
    """
    return Embedder()
